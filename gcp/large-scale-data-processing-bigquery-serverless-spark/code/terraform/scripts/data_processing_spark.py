"""
BigQuery Serverless Spark Data Processing Script
==============================================

This PySpark script demonstrates large-scale data processing capabilities
using BigQuery Serverless Spark. It processes e-commerce transaction data
and generates customer, product, and location analytics.

Features:
- Data cleaning and validation
- Customer segmentation analysis
- Product performance metrics
- Location-based analytics
- Direct BigQuery integration
- Error handling and logging

Usage:
    python data_processing_spark.py \
        --input-path gs://bucket/raw-data/sample_transactions.csv \
        --output-dataset project.dataset_name

Author: Generated by Terraform for BigQuery Serverless Spark Recipe
License: Apache 2.0
"""

import argparse
import sys
import logging
from typing import Tuple, Optional
from pyspark.sql import SparkSession, DataFrame
from pyspark.sql.functions import *
from pyspark.sql.types import *

# Configure logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)


def create_spark_session(app_name: str = "E-commerce Data Processing Pipeline") -> SparkSession:
    """
    Create optimized Spark session for BigQuery integration.
    
    Args:
        app_name: Name of the Spark application
        
    Returns:
        Configured SparkSession instance
    """
    logger.info(f"Creating Spark session: {app_name}")
    
    spark = SparkSession.builder \
        .appName(app_name) \
        .config("spark.sql.adaptive.enabled", "true") \
        .config("spark.sql.adaptive.coalescePartitions.enabled", "true") \
        .config("spark.sql.adaptive.skewJoin.enabled", "true") \
        .config("spark.sql.adaptive.localShuffleReader.enabled", "true") \
        .config("spark.serializer", "org.apache.spark.serializer.KryoSerializer") \
        .config("spark.sql.execution.arrow.pyspark.enabled", "true") \
        .getOrCreate()
    
    # Set log level to reduce verbosity
    spark.sparkContext.setLogLevel("WARN")
    
    logger.info("Spark session created successfully")
    return spark


def validate_input_data(df: DataFrame) -> bool:
    """
    Validate input data for required columns and data quality.
    
    Args:
        df: Input DataFrame to validate
        
    Returns:
        True if validation passes, False otherwise
    """
    required_columns = [
        "transaction_id", "customer_id", "product_id", 
        "product_category", "quantity", "unit_price", 
        "transaction_date", "store_location"
    ]
    
    # Check if all required columns exist
    missing_columns = set(required_columns) - set(df.columns)
    if missing_columns:
        logger.error(f"Missing required columns: {missing_columns}")
        return False
    
    # Check for empty DataFrame
    if df.count() == 0:
        logger.error("Input DataFrame is empty")
        return False
    
    logger.info("Input data validation passed")
    return True


def clean_transaction_data(df: DataFrame) -> DataFrame:
    """
    Clean and preprocess transaction data.
    
    Args:
        df: Raw transaction DataFrame
        
    Returns:
        Cleaned DataFrame with additional computed columns
    """
    logger.info("Starting data cleaning and preprocessing")
    
    # Data type conversions and cleaning
    cleaned_df = df \
        .withColumn("transaction_date", to_date(col("transaction_date"), "yyyy-MM-dd")) \
        .withColumn("quantity", col("quantity").cast("integer")) \
        .withColumn("unit_price", col("unit_price").cast("double")) \
        .withColumn("total_amount", col("quantity") * col("unit_price")) \
        .withColumn("year_month", date_format(col("transaction_date"), "yyyy-MM")) \
        .withColumn("year", year(col("transaction_date"))) \
        .withColumn("month", month(col("transaction_date"))) \
        .withColumn("day_of_week", dayofweek(col("transaction_date"))) \
        .filter(col("quantity") > 0) \
        .filter(col("unit_price") > 0) \
        .filter(col("transaction_date").isNotNull()) \
        .dropDuplicates(["transaction_id"])
    
    # Add data quality flags
    cleaned_df = cleaned_df \
        .withColumn("is_high_value", when(col("total_amount") > 1000, True).otherwise(False)) \
        .withColumn("price_category", 
                   when(col("unit_price") < 50, "Low")
                   .when(col("unit_price") < 200, "Medium")
                   .when(col("unit_price") < 500, "High")
                   .otherwise("Premium"))
    
    logger.info(f"Data cleaning completed. Rows after cleaning: {cleaned_df.count()}")
    return cleaned_df


def generate_customer_analytics(df: DataFrame) -> DataFrame:
    """
    Generate customer analytics with segmentation.
    
    Args:
        df: Cleaned transaction DataFrame
        
    Returns:
        Customer analytics DataFrame
    """
    logger.info("Generating customer analytics")
    
    customer_analytics = df \
        .groupBy("customer_id", "year_month") \
        .agg(
            count("transaction_id").alias("transaction_count"),
            sum("total_amount").alias("total_spent"),
            avg("total_amount").alias("avg_transaction_value"),
            max("total_amount").alias("max_transaction_value"),
            min("total_amount").alias("min_transaction_value"),
            countDistinct("product_category").alias("unique_categories"),
            countDistinct("store_location").alias("unique_locations"),
            sum("quantity").alias("total_items_purchased"),
            first("year").alias("year"),
            first("month").alias("month")
        ) \
        .withColumn("customer_segment", 
                   when(col("total_spent") > 2000, "Premium")
                   .when(col("total_spent") > 1000, "Gold")
                   .when(col("total_spent") > 500, "Silver")
                   .otherwise("Bronze")) \
        .withColumn("avg_items_per_transaction", 
                   col("total_items_purchased") / col("transaction_count")) \
        .withColumn("spending_consistency", 
                   when(col("max_transaction_value") / col("min_transaction_value") < 2, "Consistent")
                   .when(col("max_transaction_value") / col("min_transaction_value") < 5, "Moderate")
                   .otherwise("Variable"))
    
    logger.info("Customer analytics generation completed")
    return customer_analytics


def generate_product_analytics(df: DataFrame) -> DataFrame:
    """
    Generate product performance analytics.
    
    Args:
        df: Cleaned transaction DataFrame
        
    Returns:
        Product analytics DataFrame
    """
    logger.info("Generating product analytics")
    
    product_analytics = df \
        .groupBy("product_category", "year_month") \
        .agg(
            sum("quantity").alias("total_quantity_sold"),
            sum("total_amount").alias("category_revenue"),
            countDistinct("customer_id").alias("unique_customers"),
            countDistinct("store_location").alias("unique_locations"),
            avg("unit_price").alias("avg_unit_price"),
            max("unit_price").alias("max_unit_price"),
            min("unit_price").alias("min_unit_price"),
            count("transaction_id").alias("transaction_count"),
            first("year").alias("year"),
            first("month").alias("month")
        ) \
        .withColumn("revenue_per_customer", 
                   col("category_revenue") / col("unique_customers")) \
        .withColumn("avg_quantity_per_transaction", 
                   col("total_quantity_sold") / col("transaction_count")) \
        .withColumn("price_variance", 
                   col("max_unit_price") - col("min_unit_price")) \
        .withColumn("market_penetration", 
                   col("unique_locations") / 10.0)  # Assuming 10 total locations
    
    logger.info("Product analytics generation completed")
    return product_analytics


def generate_location_analytics(df: DataFrame) -> DataFrame:
    """
    Generate location-based analytics.
    
    Args:
        df: Cleaned transaction DataFrame
        
    Returns:
        Location analytics DataFrame
    """
    logger.info("Generating location analytics")
    
    location_analytics = df \
        .groupBy("store_location", "product_category", "year_month") \
        .agg(
            sum("total_amount").alias("location_category_revenue"),
            count("transaction_id").alias("transaction_volume"),
            sum("quantity").alias("total_quantity"),
            countDistinct("customer_id").alias("unique_customers"),
            avg("total_amount").alias("avg_transaction_value"),
            first("year").alias("year"),
            first("month").alias("month")
        ) \
        .withColumn("revenue_per_transaction", 
                   col("location_category_revenue") / col("transaction_volume")) \
        .withColumn("items_per_transaction", 
                   col("total_quantity") / col("transaction_volume")) \
        .withColumn("customer_frequency", 
                   col("transaction_volume") / col("unique_customers"))
    
    # Add location performance ranking
    location_window = Window.partitionBy("year_month").orderBy(desc("location_category_revenue"))
    location_analytics = location_analytics \
        .withColumn("revenue_rank", row_number().over(location_window))
    
    logger.info("Location analytics generation completed")
    return location_analytics


def write_to_bigquery(df: DataFrame, table_name: str, output_dataset: str, write_mode: str = "overwrite") -> None:
    """
    Write DataFrame to BigQuery table.
    
    Args:
        df: DataFrame to write
        table_name: Name of the BigQuery table
        output_dataset: BigQuery dataset in format project.dataset
        write_mode: Write mode (overwrite, append)
    """
    logger.info(f"Writing {df.count()} rows to BigQuery table: {output_dataset}.{table_name}")
    
    try:
        df.write \
            .format("bigquery") \
            .option("table", f"{output_dataset}.{table_name}") \
            .option("writeMethod", "direct") \
            .option("createDisposition", "CREATE_IF_NEEDED") \
            .mode(write_mode) \
            .save()
        
        logger.info(f"Successfully wrote data to {output_dataset}.{table_name}")
        
    except Exception as e:
        logger.error(f"Failed to write to BigQuery table {table_name}: {str(e)}")
        raise


def process_transaction_data(spark: SparkSession, input_path: str, output_dataset: str) -> Tuple[DataFrame, DataFrame, DataFrame]:
    """
    Main data processing function that orchestrates the entire pipeline.
    
    Args:
        spark: SparkSession instance
        input_path: Path to input data in Cloud Storage
        output_dataset: BigQuery dataset for output tables
        
    Returns:
        Tuple of (customer_analytics, product_analytics, location_analytics) DataFrames
    """
    logger.info(f"Starting data processing pipeline")
    logger.info(f"Input path: {input_path}")
    logger.info(f"Output dataset: {output_dataset}")
    
    # Read input data
    logger.info("Reading input data from Cloud Storage")
    df = spark.read \
        .option("header", "true") \
        .option("inferSchema", "true") \
        .csv(input_path)
    
    initial_count = df.count()
    logger.info(f"Loaded {initial_count} transactions for processing")
    
    # Validate input data
    if not validate_input_data(df):
        raise ValueError("Input data validation failed")
    
    # Clean and preprocess data
    cleaned_df = clean_transaction_data(df)
    
    # Cache the cleaned data since it will be used multiple times
    cleaned_df.cache()
    
    # Generate analytics
    customer_analytics = generate_customer_analytics(cleaned_df)
    product_analytics = generate_product_analytics(cleaned_df)
    location_analytics = generate_location_analytics(cleaned_df)
    
    # Write results to BigQuery
    write_to_bigquery(customer_analytics, "customer_analytics", output_dataset)
    write_to_bigquery(product_analytics, "product_analytics", output_dataset)
    write_to_bigquery(location_analytics, "location_analytics", output_dataset)
    
    logger.info("âœ… All analytics tables successfully written to BigQuery")
    
    # Show sample results for verification
    logger.info("=== Sample Results ===")
    logger.info("Customer Analytics Sample:")
    customer_analytics.show(5, truncate=False)
    
    logger.info("Product Analytics Sample:")
    product_analytics.show(5, truncate=False)
    
    logger.info("Location Analytics Sample:")
    location_analytics.show(5, truncate=False)
    
    return customer_analytics, product_analytics, location_analytics


def parse_arguments() -> argparse.Namespace:
    """
    Parse command line arguments.
    
    Returns:
        Parsed arguments namespace
    """
    parser = argparse.ArgumentParser(
        description="BigQuery Serverless Spark Data Processing Pipeline",
        formatter_class=argparse.RawDescriptionHelpFormatter
    )
    
    parser.add_argument(
        "--input-path",
        required=True,
        help="Path to input CSV file in Cloud Storage (e.g., gs://bucket/data.csv)"
    )
    
    parser.add_argument(
        "--output-dataset",
        required=True,
        help="BigQuery output dataset in format project.dataset (e.g., myproject.analytics)"
    )
    
    parser.add_argument(
        "--app-name",
        default="E-commerce Data Processing Pipeline",
        help="Spark application name"
    )
    
    parser.add_argument(
        "--log-level",
        choices=["DEBUG", "INFO", "WARN", "ERROR"],
        default="INFO",
        help="Logging level"
    )
    
    return parser.parse_args()


def main():
    """
    Main function that orchestrates the data processing pipeline.
    """
    try:
        # Parse command line arguments
        args = parse_arguments()
        
        # Set logging level
        logging.getLogger().setLevel(getattr(logging, args.log_level))
        
        logger.info("Starting BigQuery Serverless Spark Data Processing Pipeline")
        logger.info(f"Arguments: {vars(args)}")
        
        # Create Spark session
        spark = create_spark_session(args.app_name)
        
        # Process data
        customer_df, product_df, location_df = process_transaction_data(
            spark, args.input_path, args.output_dataset
        )
        
        logger.info("Data processing pipeline completed successfully")
        
    except Exception as e:
        logger.error(f"Pipeline failed with error: {str(e)}")
        sys.exit(1)
        
    finally:
        # Clean up Spark session
        if 'spark' in locals():
            logger.info("Stopping Spark session")
            spark.stop()


if __name__ == "__main__":
    main()