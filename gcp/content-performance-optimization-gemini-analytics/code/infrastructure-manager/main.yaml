# Infrastructure Manager Configuration for Content Performance Optimization using Gemini and Analytics
# This configuration deploys a complete content optimization system that analyzes performance data
# in BigQuery, uses Vertex AI Gemini 2.5 Flash to identify improvement patterns, and generates
# data-driven content variations through serverless Cloud Functions.

imports:
  - path: content-analysis-function.zip
    name: content-analysis-function

# Variables for customizing the deployment
variables:
  # Project configuration
  project_id:
    type: string
    description: "Google Cloud project ID where resources will be deployed"
    required: true

  region:
    type: string
    description: "Google Cloud region for resource deployment"
    default: "us-central1"

  # Resource naming configuration
  dataset_name:
    type: string
    description: "BigQuery dataset name for content analytics"
    default: "content_analytics"

  bucket_name:
    type: string
    description: "Cloud Storage bucket name for content optimization assets"
    required: true

  function_name:
    type: string
    description: "Cloud Function name for content analysis engine"
    default: "content-analyzer"

  # Performance and cost optimization
  function_memory:
    type: string
    description: "Memory allocation for Cloud Function (128MB to 8GB)"
    default: "512MB"

  function_timeout:
    type: string
    description: "Timeout for Cloud Function execution (1s to 540s)"
    default: "300s"

  # Security and access control
  allow_unauthenticated:
    type: boolean
    description: "Allow unauthenticated access to Cloud Function (set false for production)"
    default: true

# Required Google Cloud APIs for the content optimization system
resources:
  # Enable required APIs for the project
  - name: enable-apis
    type: gcp-types/serviceusage-v1:services
    properties:
      name: projects/$(ref.project_id.name)/services/cloudfunctions.googleapis.com
      consumerId: project:$(ref.project_id.name)
    metadata:
      dependsOn:
        - project_id

  - name: enable-vertex-ai-api
    type: gcp-types/serviceusage-v1:services
    properties:
      name: projects/$(ref.project_id.name)/services/aiplatform.googleapis.com
      consumerId: project:$(ref.project_id.name)
    metadata:
      dependsOn:
        - project_id

  - name: enable-bigquery-api
    type: gcp-types/serviceusage-v1:services
    properties:
      name: projects/$(ref.project_id.name)/services/bigquery.googleapis.com
      consumerId: project:$(ref.project_id.name)
    metadata:
      dependsOn:
        - project_id

  - name: enable-storage-api
    type: gcp-types/serviceusage-v1:services
    properties:
      name: projects/$(ref.project_id.name)/services/storage.googleapis.com
      consumerId: project:$(ref.project_id.name)
    metadata:
      dependsOn:
        - project_id

  - name: enable-cloudbuild-api
    type: gcp-types/serviceusage-v1:services
    properties:
      name: projects/$(ref.project_id.name)/services/cloudbuild.googleapis.com
      consumerId: project:$(ref.project_id.name)
    metadata:
      dependsOn:
        - project_id

  # BigQuery Dataset for content analytics data
  - name: content-analytics-dataset
    type: gcp-types/bigquery-v2:datasets
    properties:
      projectId: $(ref.project_id.name)
      datasetId: $(ref.dataset_name)
      location: $(ref.region)
      description: "Dataset for storing content performance metrics and analytics data"
      labels:
        purpose: "content-optimization"
        environment: "analytics"
        cost-center: "marketing"
      # Access control for the dataset
      access:
        - role: "OWNER"
          specialGroup: "projectOwners"
        - role: "READER"
          specialGroup: "projectReaders"
        - role: "WRITER"
          userByEmail: "$(ref.project_id.name)@appspot.gserviceaccount.com"
    metadata:
      dependsOn:
        - enable-bigquery-api

  # Performance data table with comprehensive schema for content metrics
  - name: performance-data-table
    type: gcp-types/bigquery-v2:tables
    properties:
      projectId: $(ref.project_id.name)
      datasetId: $(ref.content-analytics-dataset.datasetId)
      tableId: "performance_data"
      description: "Table storing content performance metrics for AI-driven analysis"
      labels:
        data-type: "performance-metrics"
        source: "content-analytics"
      # Comprehensive schema for content performance data
      schema:
        fields:
          - name: "content_id"
            type: "STRING"
            mode: "REQUIRED"
            description: "Unique identifier for content pieces"
          - name: "title"
            type: "STRING"
            mode: "REQUIRED"
            description: "Content title or headline"
          - name: "content_type"
            type: "STRING"
            mode: "REQUIRED"
            description: "Type of content (blog, video, infographic, case_study, etc.)"
          - name: "publish_date"
            type: "TIMESTAMP"
            mode: "REQUIRED"
            description: "Content publication timestamp"
          - name: "page_views"
            type: "INTEGER"
            mode: "NULLABLE"
            description: "Total page views for the content"
          - name: "engagement_rate"
            type: "FLOAT"
            mode: "NULLABLE"
            description: "User engagement rate (0.0 to 1.0)"
          - name: "conversion_rate"
            type: "FLOAT"
            mode: "NULLABLE"
            description: "Conversion rate from content (0.0 to 1.0)"
          - name: "time_on_page"
            type: "FLOAT"
            mode: "NULLABLE"
            description: "Average time spent on content in seconds"
          - name: "social_shares"
            type: "INTEGER"
            mode: "NULLABLE"
            description: "Total social media shares"
          - name: "content_score"
            type: "FLOAT"
            mode: "NULLABLE"
            description: "Overall content performance score (0.0 to 10.0)"
      # Time partitioning for query performance optimization
      timePartitioning:
        type: "DAY"
        field: "publish_date"
        requirePartitionFilter: false
      # Clustering for query optimization
      clustering:
        fields:
          - "content_type"
          - "content_score"
    metadata:
      dependsOn:
        - content-analytics-dataset

  # BigQuery view for aggregated performance metrics by content type
  - name: content-performance-summary-view
    type: gcp-types/bigquery-v2:tables
    properties:
      projectId: $(ref.project_id.name)
      datasetId: $(ref.content-analytics-dataset.datasetId)
      tableId: "content_performance_summary"
      description: "Aggregated view of content performance metrics by type"
      labels:
        view-type: "performance-summary"
        purpose: "analytics-dashboard"
      # View definition for performance aggregations
      view:
        query: |
          SELECT 
            content_type,
            COUNT(*) as total_content,
            AVG(page_views) as avg_page_views,
            AVG(engagement_rate) as avg_engagement,
            AVG(conversion_rate) as avg_conversion,
            AVG(content_score) as avg_score,
            SUM(social_shares) as total_shares,
            MAX(publish_date) as latest_publish_date,
            MIN(publish_date) as earliest_publish_date
          FROM `$(ref.project_id.name).$(ref.content-analytics-dataset.datasetId).performance_data`
          GROUP BY content_type
          ORDER BY avg_score DESC
        useLegacySql: false
    metadata:
      dependsOn:
        - performance-data-table

  # BigQuery view for content rankings and performance categories
  - name: content-rankings-view
    type: gcp-types/bigquery-v2:tables
    properties:
      projectId: $(ref.project_id.name)
      datasetId: $(ref.content-analytics-dataset.datasetId)
      tableId: "content_rankings"
      description: "Content rankings with performance categories for optimization insights"
      labels:
        view-type: "content-rankings"
        purpose: "performance-analysis"
      # View definition for content rankings and categorization
      view:
        query: |
          SELECT 
            content_id,
            title,
            content_type,
            content_score,
            page_views,
            engagement_rate,
            conversion_rate,
            publish_date,
            CASE 
              WHEN content_score >= 9.0 THEN 'Excellent'
              WHEN content_score >= 7.5 THEN 'Good'
              WHEN content_score >= 6.0 THEN 'Average'
              ELSE 'Needs Improvement'
            END as performance_category,
            RANK() OVER (PARTITION BY content_type ORDER BY content_score DESC) as type_rank,
            RANK() OVER (ORDER BY content_score DESC) as overall_rank
          FROM `$(ref.project_id.name).$(ref.content-analytics-dataset.datasetId).performance_data`
          ORDER BY content_score DESC
        useLegacySql: false
    metadata:
      dependsOn:
        - performance-data-table

  # Cloud Storage bucket for content assets and analysis results
  - name: content-optimization-bucket
    type: gcp-types/storage-v1:buckets
    properties:
      name: $(ref.bucket_name)
      project: $(ref.project_id.name)
      location: $(ref.region)
      storageClass: "STANDARD"
      # Enable versioning for content asset protection and audit trails
      versioning:
        enabled: true
      # Lifecycle management for cost optimization
      lifecycle:
        rule:
          - action:
              type: "SetStorageClass"
              storageClass: "NEARLINE"
            condition:
              age: 30
              matchesStorageClass: ["STANDARD"]
          - action:
              type: "SetStorageClass"
              storageClass: "COLDLINE"
            condition:
              age: 90
              matchesStorageClass: ["NEARLINE"]
          - action:
              type: "Delete"
            condition:
              age: 365
              matchesStorageClass: ["COLDLINE"]
      # Security and access logging
      logging:
        logBucket: $(ref.bucket_name)
        logObjectPrefix: "access-logs/"
      # Labels for resource management
      labels:
        purpose: "content-optimization"
        data-type: "analytics-results"
        cost-center: "marketing"
    metadata:
      dependsOn:
        - enable-storage-api

  # IAM binding for Cloud Function to access Cloud Storage bucket
  - name: bucket-iam-binding
    type: gcp-types/storage-v1:buckets.setIamPolicy
    properties:
      bucket: $(ref.content-optimization-bucket.name)
      project: $(ref.project_id.name)
      policy:
        bindings:
          - role: "roles/storage.objectAdmin"
            members:
              - "serviceAccount:$(ref.project_id.name)@appspot.gserviceaccount.com"
          - role: "roles/storage.legacyBucketReader"
            members:
              - "serviceAccount:$(ref.project_id.name)@appspot.gserviceaccount.com"
    metadata:
      dependsOn:
        - content-optimization-bucket

  # Service account for Cloud Function with required permissions
  - name: content-analyzer-service-account
    type: gcp-types/iam-v1:projects.serviceAccounts
    properties:
      projectId: $(ref.project_id.name)
      accountId: "content-analyzer-sa"
      serviceAccount:
        displayName: "Content Analyzer Service Account"
        description: "Service account for Cloud Function performing content analysis with Vertex AI"
    metadata:
      dependsOn:
        - enable-apis

  # IAM bindings for service account to access required services
  - name: bigquery-user-binding
    type: gcp-types/cloudresourcemanager-v1:projects.setIamPolicy
    properties:
      resource: $(ref.project_id.name)
      policy:
        bindings:
          - role: "roles/bigquery.user"
            members:
              - "serviceAccount:$(ref.content-analyzer-service-account.email)"
          - role: "roles/bigquery.dataViewer"
            members:
              - "serviceAccount:$(ref.content-analyzer-service-account.email)"
          - role: "roles/aiplatform.user"
            members:
              - "serviceAccount:$(ref.content-analyzer-service-account.email)"
          - role: "roles/storage.objectAdmin"
            members:
              - "serviceAccount:$(ref.content-analyzer-service-account.email)"
    metadata:
      dependsOn:
        - content-analyzer-service-account

  # Cloud Function for content analysis with Vertex AI Gemini integration
  - name: content-analysis-function
    type: gcp-types/cloudfunctions-v2:projects.locations.functions
    properties:
      parent: projects/$(ref.project_id.name)/locations/$(ref.region)
      functionId: $(ref.function_name)
      function:
        description: "Serverless function for analyzing content performance using Vertex AI Gemini 2.5 Flash"
        labels:
          purpose: "content-analysis"
          ai-model: "gemini-flash"
          cost-center: "marketing"
        # Build configuration for Python runtime
        buildConfig:
          runtime: "python312"
          entryPoint: "analyze_content"
          # Source code from uploaded zip file
          source:
            storageSource:
              bucket: $(ref.content-optimization-bucket.name)
              object: "content-analysis-function.zip"
        # Service configuration with optimized resources
        serviceConfig:
          maxInstanceCount: 100
          minInstanceCount: 0
          availableMemory: $(ref.function_memory)
          timeoutSeconds: $(ref.function_timeout)
          maxInstanceRequestConcurrency: 80
          serviceAccountEmail: $(ref.content-analyzer-service-account.email)
          # Environment variables for service integration
          environmentVariables:
            GCP_PROJECT: $(ref.project_id.name)
            DATASET_NAME: $(ref.content-analytics-dataset.datasetId)
            BUCKET_NAME: $(ref.content-optimization-bucket.name)
            VERTEX_AI_LOCATION: $(ref.region)
          # VPC connector for private network access (optional)
          # vpcConnector: "projects/$(ref.project_id.name)/locations/$(ref.region)/connectors/content-vpc"
          # ingressSettings: "ALLOW_INTERNAL_ONLY"
        # Event trigger configuration for HTTP requests
        eventTrigger:
          triggerRegion: $(ref.region)
          eventType: "google.cloud.pubsub.topic.v1.messagePublished"
          pubsubTopic: "projects/$(ref.project_id.name)/topics/content-analysis-trigger"
          retryPolicy: "RETRY_POLICY_RETRY"
    metadata:
      dependsOn:
        - content-optimization-bucket
        - content-analyzer-service-account
        - enable-apis

  # HTTP trigger for Cloud Function (alternative to Pub/Sub)
  - name: content-analysis-http-function
    type: gcp-types/cloudfunctions-v2:projects.locations.functions
    properties:
      parent: projects/$(ref.project_id.name)/locations/$(ref.region)
      functionId: "$(ref.function_name)-http"
      function:
        description: "HTTP-triggered function for content analysis with Vertex AI Gemini"
        labels:
          purpose: "content-analysis-http"
          trigger-type: "http"
        buildConfig:
          runtime: "python312"
          entryPoint: "analyze_content"
          source:
            storageSource:
              bucket: $(ref.content-optimization-bucket.name)
              object: "content-analysis-function.zip"
        serviceConfig:
          maxInstanceCount: 50
          availableMemory: $(ref.function_memory)
          timeoutSeconds: $(ref.function_timeout)
          serviceAccountEmail: $(ref.content-analyzer-service-account.email)
          environmentVariables:
            GCP_PROJECT: $(ref.project_id.name)
            DATASET_NAME: $(ref.content-analytics-dataset.datasetId)
            BUCKET_NAME: $(ref.content-optimization-bucket.name)
            VERTEX_AI_LOCATION: $(ref.region)
          # Allow unauthenticated access for demo purposes (configure as needed)
          ingressSettings: "ALLOW_ALL"
    metadata:
      dependsOn:
        - content-optimization-bucket
        - content-analyzer-service-account

  # IAM policy for Cloud Function HTTP access
  - name: function-invoker-policy
    type: gcp-types/cloudfunctions-v2:projects.locations.functions.setIamPolicy
    properties:
      resource: $(ref.content-analysis-http-function.name)
      policy:
        bindings:
          - role: "roles/cloudfunctions.invoker"
            members:
              - "allUsers"
    metadata:
      dependsOn:
        - content-analysis-http-function
    condition: $(ref.allow_unauthenticated)

  # Pub/Sub topic for triggering content analysis (for scheduled or event-driven analysis)
  - name: content-analysis-topic
    type: gcp-types/pubsub-v1:projects.topics
    properties:
      name: "projects/$(ref.project_id.name)/topics/content-analysis-trigger"
      labels:
        purpose: "content-analysis-trigger"
        system: "optimization-pipeline"
    metadata:
      dependsOn:
        - enable-apis

  # Cloud Scheduler job for automated content analysis (optional)
  - name: content-analysis-scheduler
    type: gcp-types/cloudscheduler-v1:projects.locations.jobs
    properties:
      parent: "projects/$(ref.project_id.name)/locations/$(ref.region)"
      job:
        name: "content-analysis-daily"
        description: "Daily automated content performance analysis"
        schedule: "0 9 * * *"  # Daily at 9 AM UTC
        timeZone: "UTC"
        # HTTP target for triggering the function
        httpTarget:
          uri: $(ref.content-analysis-http-function.serviceConfig.uri)
          httpMethod: "POST"
          headers:
            Content-Type: "application/json"
          body: '{"trigger": "scheduled_analysis", "source": "cloud_scheduler"}'
          # OAuth token for authentication
          oauthToken:
            serviceAccountEmail: $(ref.content-analyzer-service-account.email)
            scope: "https://www.googleapis.com/auth/cloud-platform"
        retryConfig:
          retryCount: 3
          maxRetryDuration: "600s"
          minBackoffDuration: "5s"
          maxBackoffDuration: "3600s"
    metadata:
      dependsOn:
        - content-analysis-http-function

  # Cloud Monitoring alert policy for function errors
  - name: function-error-alert
    type: gcp-types/monitoring-v1:projects.alertPolicies
    properties:
      name: "projects/$(ref.project_id.name)/alertPolicies/content-analysis-errors"
      displayName: "Content Analysis Function Errors"
      documentation:
        content: "Alert when content analysis function experiences high error rates"
        mimeType: "text/markdown"
      conditions:
        - displayName: "Function error rate too high"
          conditionThreshold:
            filter: 'resource.type="cloud_function" resource.label.function_name="$(ref.function_name)-http"'
            comparison: "COMPARISON_GREATER_THAN"
            thresholdValue: 0.1  # 10% error rate
            duration: "300s"
            aggregations:
              - alignmentPeriod: "60s"
                perSeriesAligner: "ALIGN_RATE"
                crossSeriesReducer: "REDUCE_MEAN"
                groupByFields:
                  - "resource.label.function_name"
      alertStrategy:
        autoClose: "1800s"  # Auto-close after 30 minutes
      enabled: true
      notificationChannels: []  # Add notification channels as needed
    metadata:
      dependsOn:
        - content-analysis-http-function

# Outputs for integration and verification
outputs:
  # BigQuery dataset and table information
  - name: bigquery_dataset_id
    description: "BigQuery dataset ID for content analytics"
    value: $(ref.content-analytics-dataset.datasetId)

  - name: performance_table_id
    description: "BigQuery table ID for performance data"
    value: $(ref.performance-data-table.tableId)

  - name: bigquery_location
    description: "BigQuery dataset location"
    value: $(ref.content-analytics-dataset.location)

  # Cloud Storage bucket information
  - name: storage_bucket_name
    description: "Cloud Storage bucket name for content optimization"
    value: $(ref.content-optimization-bucket.name)

  - name: storage_bucket_url
    description: "Cloud Storage bucket URL"
    value: "gs://$(ref.content-optimization-bucket.name)"

  # Cloud Function information
  - name: function_name
    description: "Content analysis Cloud Function name"
    value: $(ref.content-analysis-http-function.name)

  - name: function_url
    description: "HTTP trigger URL for content analysis function"
    value: $(ref.content-analysis-http-function.serviceConfig.uri)

  - name: function_service_account
    description: "Service account email for Cloud Function"
    value: $(ref.content-analyzer-service-account.email)

  # BigQuery view information
  - name: performance_summary_view
    description: "BigQuery view for performance summary"
    value: "$(ref.project_id.name).$(ref.content-analytics-dataset.datasetId).content_performance_summary"

  - name: content_rankings_view
    description: "BigQuery view for content rankings"
    value: "$(ref.project_id.name).$(ref.content-analytics-dataset.datasetId).content_rankings"

  # Pub/Sub and scheduling information
  - name: pubsub_topic
    description: "Pub/Sub topic for triggering analysis"
    value: $(ref.content-analysis-topic.name)

  - name: scheduler_job
    description: "Cloud Scheduler job for automated analysis"
    value: $(ref.content-analysis-scheduler.name)

  # Sample data loading commands
  - name: sample_data_commands
    description: "Commands to load sample data into BigQuery"
    value: |
      # Load sample content performance data
      echo '{"content_id": "blog_001", "title": "10 Tips for Better SEO", "content_type": "blog", "publish_date": "2024-01-15 10:00:00", "page_views": 1250, "engagement_rate": 0.65, "conversion_rate": 0.08, "time_on_page": 180.5, "social_shares": 45, "content_score": 8.2}
      {"content_id": "video_001", "title": "Product Demo Walkthrough", "content_type": "video", "publish_date": "2024-01-20 14:30:00", "page_views": 890, "engagement_rate": 0.78, "conversion_rate": 0.12, "time_on_page": 220.3, "social_shares": 67, "content_score": 9.1}
      {"content_id": "infographic_001", "title": "Data Visualization Best Practices", "content_type": "infographic", "publish_date": "2024-01-25 09:15:00", "page_views": 2100, "engagement_rate": 0.72, "conversion_rate": 0.05, "time_on_page": 95.2, "social_shares": 89, "content_score": 7.8}
      {"content_id": "blog_002", "title": "Advanced Analytics Techniques", "content_type": "blog", "publish_date": "2024-02-01 11:45:00", "page_views": 780, "engagement_rate": 0.58, "conversion_rate": 0.06, "time_on_page": 165.7, "social_shares": 23, "content_score": 6.9}
      {"content_id": "case_study_001", "title": "Customer Success Story", "content_type": "case_study", "publish_date": "2024-02-05 16:20:00", "page_views": 1450, "engagement_rate": 0.82, "conversion_rate": 0.18, "time_on_page": 310.8, "social_shares": 78, "content_score": 9.5}' > sample_data.json
      
      bq load --source_format=NEWLINE_DELIMITED_JSON $(ref.project_id.name):$(ref.content-analytics-dataset.datasetId).performance_data sample_data.json

  # Usage instructions
  - name: getting_started
    description: "Instructions for using the content optimization system"
    value: |
      # Content Performance Optimization System - Getting Started
      
      1. Load sample data using the provided commands above
      2. Test the analysis function: curl -X POST $(ref.content-analysis-http-function.serviceConfig.uri) -H "Content-Type: application/json" -d '{"trigger": "manual_analysis"}'
      3. Query performance summary: bq query "SELECT * FROM \`$(ref.project_id.name).$(ref.content-analytics-dataset.datasetId).content_performance_summary\`"
      4. View content rankings: bq query "SELECT * FROM \`$(ref.project_id.name).$(ref.content-analytics-dataset.datasetId).content_rankings\` LIMIT 10"
      5. Check analysis results: gsutil ls gs://$(ref.content-optimization-bucket.name)/analysis_results/
      
      # Monitor costs and usage
      - Set up BigQuery slot reservations for predictable workloads
      - Configure Vertex AI quotas in the Google Cloud Console
      - Monitor Cloud Function execution costs and optimize memory allocation
      
      # Security considerations
      - Review IAM permissions and adjust as needed for production
      - Configure VPC connector for private network access if required
      - Enable audit logging for compliance requirements