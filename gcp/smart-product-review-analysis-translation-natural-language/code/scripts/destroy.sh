#!/bin/bash

# Destroy script for Smart Product Review Analysis with Translation and Natural Language AI
# Recipe: smart-product-review-analysis-translation-natural-language
# Provider: GCP
# Generated by: recipe-generator-version 1.3

set -euo pipefail

# Script configuration
SCRIPT_DIR="$(cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)"
LOG_FILE="${SCRIPT_DIR}/destroy.log"
ERROR_LOG="${SCRIPT_DIR}/destroy_errors.log"
DEPLOYMENT_INFO="${SCRIPT_DIR}/deployment_info.env"

# Color codes for output
RED='\033[0;31m'
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
BLUE='\033[0;34m'
NC='\033[0m' # No Color

# Logging functions
log() {
    echo -e "${GREEN}[$(date +'%Y-%m-%d %H:%M:%S')]${NC} $1" | tee -a "$LOG_FILE"
}

warn() {
    echo -e "${YELLOW}[$(date +'%Y-%m-%d %H:%M:%S')] WARNING:${NC} $1" | tee -a "$LOG_FILE"
}

error() {
    echo -e "${RED}[$(date +'%Y-%m-%d %H:%M:%S')] ERROR:${NC} $1" | tee -a "$ERROR_LOG"
}

info() {
    echo -e "${BLUE}[$(date +'%Y-%m-%d %H:%M:%S')] INFO:${NC} $1" | tee -a "$LOG_FILE"
}

# Function to load deployment information
load_deployment_info() {
    info "Loading deployment information..."
    
    if [[ -f "$DEPLOYMENT_INFO" ]]; then
        # Source the deployment info file
        set -a  # automatically export all variables
        source "$DEPLOYMENT_INFO"
        set +a  # turn off automatic export
        
        log "Loaded deployment information:"
        log "  PROJECT_ID: ${PROJECT_ID:-not set}"
        log "  REGION: ${REGION:-not set}"
        log "  DATASET_NAME: ${DATASET_NAME:-not set}"
        log "  FUNCTION_NAME: ${FUNCTION_NAME:-not set}"
    else
        warn "Deployment info file not found at $DEPLOYMENT_INFO"
        warn "You may need to provide resource names manually"
        
        # Try to get from environment or prompt user
        if [[ -z "${PROJECT_ID:-}" ]]; then
            read -p "Enter PROJECT_ID: " PROJECT_ID
            export PROJECT_ID
        fi
        
        if [[ -z "${REGION:-}" ]]; then
            export REGION="${REGION:-us-central1}"
            warn "Using default region: $REGION"
        fi
        
        if [[ -z "${DATASET_NAME:-}" ]]; then
            warn "DATASET_NAME not provided. Skipping BigQuery cleanup."
            warn "You may need to manually delete BigQuery datasets starting with 'product_reviews_'"
        fi
        
        if [[ -z "${FUNCTION_NAME:-}" ]]; then
            warn "FUNCTION_NAME not provided. Skipping Cloud Function cleanup."
            warn "You may need to manually delete Cloud Functions starting with 'review-analysis-'"
        fi
    fi
}

# Function to check prerequisites
check_prerequisites() {
    info "Checking prerequisites..."
    
    # Check if gcloud CLI is installed
    if ! command -v gcloud &> /dev/null; then
        error "Google Cloud CLI (gcloud) is not installed."
        exit 1
    fi
    
    # Check if bq CLI is available
    if ! command -v bq &> /dev/null; then
        error "BigQuery CLI (bq) is not installed."
        exit 1
    fi
    
    # Check if user is authenticated with gcloud
    if ! gcloud auth list --filter=status:ACTIVE --format="value(account)" | grep -q .; then
        error "No active gcloud authentication found. Please run 'gcloud auth login' first."
        exit 1
    fi
    
    log "Prerequisites check completed successfully"
}

# Function to confirm destruction
confirm_destruction() {
    echo ""
    warn "============================================"
    warn "DESTRUCTIVE OPERATION WARNING"
    warn "============================================"
    warn "This script will permanently delete:"
    warn "  - Cloud Function: ${FUNCTION_NAME:-'review-analysis-*'}"
    warn "  - BigQuery Dataset: ${DATASET_NAME:-'product_reviews_*'} (and all data)"
    warn "  - Local function code and sample data"
    warn ""
    warn "This action CANNOT be undone!"
    warn "============================================"
    echo ""
    
    if [[ "${FORCE_DESTROY:-}" == "true" ]]; then
        warn "Force destroy mode enabled, skipping confirmation"
        return 0
    fi
    
    read -p "Are you sure you want to proceed? Type 'DELETE' to confirm: " confirmation
    
    if [[ "$confirmation" != "DELETE" ]]; then
        info "Destruction cancelled by user"
        exit 0
    fi
    
    log "User confirmed destruction, proceeding..."
}

# Function to set project context
set_project_context() {
    if [[ -n "${PROJECT_ID:-}" ]]; then
        info "Setting project context to: $PROJECT_ID"
        gcloud config set project "${PROJECT_ID}" 2>/dev/null || {
            error "Failed to set project ${PROJECT_ID}. Please check if the project exists and you have access."
            exit 1
        }
        
        if [[ -n "${REGION:-}" ]]; then
            gcloud config set compute/region "${REGION}" 2>/dev/null
        fi
    else
        error "PROJECT_ID not available. Cannot proceed with destruction."
        exit 1
    fi
}

# Function to delete Cloud Function
delete_cloud_function() {
    if [[ -n "${FUNCTION_NAME:-}" ]]; then
        info "Deleting Cloud Function: $FUNCTION_NAME"
        
        # Check if function exists
        if gcloud functions describe "${FUNCTION_NAME}" \
            --region="${REGION}" \
            --format="value(name)" &>/dev/null; then
            
            if gcloud functions delete "${FUNCTION_NAME}" \
                --region="${REGION}" \
                --quiet; then
                log "Cloud Function deleted successfully"
            else
                error "Failed to delete Cloud Function: $FUNCTION_NAME"
                return 1
            fi
        else
            warn "Cloud Function $FUNCTION_NAME not found or already deleted"
        fi
    else
        warn "FUNCTION_NAME not provided, skipping Cloud Function deletion"
        
        # Try to find and delete functions that match the pattern
        info "Searching for functions with pattern 'review-analysis-*'..."
        
        functions=$(gcloud functions list \
            --regions="${REGION}" \
            --filter="name:review-analysis-*" \
            --format="value(name)" 2>/dev/null || true)
        
        if [[ -n "$functions" ]]; then
            warn "Found functions matching pattern:"
            echo "$functions"
            echo ""
            read -p "Delete these functions? (y/N): " delete_confirm
            
            if [[ "$delete_confirm" =~ ^[Yy]$ ]]; then
                for func in $functions; do
                    info "Deleting function: $func"
                    gcloud functions delete "$func" \
                        --region="${REGION}" \
                        --quiet || warn "Failed to delete $func"
                done
            else
                warn "Skipped Cloud Function deletion"
            fi
        else
            info "No functions found matching pattern 'review-analysis-*'"
        fi
    fi
}

# Function to delete BigQuery resources
delete_bigquery_resources() {
    if [[ -n "${DATASET_NAME:-}" ]]; then
        info "Deleting BigQuery dataset: $DATASET_NAME"
        
        # Check if dataset exists
        if bq ls -d "${PROJECT_ID}:${DATASET_NAME}" &>/dev/null; then
            if bq rm -r -f "${PROJECT_ID}:${DATASET_NAME}"; then
                log "BigQuery dataset deleted successfully"
            else
                error "Failed to delete BigQuery dataset: $DATASET_NAME"
                return 1
            fi
        else
            warn "BigQuery dataset $DATASET_NAME not found or already deleted"
        fi
    else
        warn "DATASET_NAME not provided, skipping BigQuery deletion"
        
        # Try to find and delete datasets that match the pattern
        info "Searching for datasets with pattern 'product_reviews_*'..."
        
        datasets=$(bq ls --filter="datasetId:product_reviews_*" \
            --format="value(datasetId)" 2>/dev/null || true)
        
        if [[ -n "$datasets" ]]; then
            warn "Found datasets matching pattern:"
            echo "$datasets"
            echo ""
            read -p "Delete these datasets? (y/N): " delete_confirm
            
            if [[ "$delete_confirm" =~ ^[Yy]$ ]]; then
                for dataset in $datasets; do
                    info "Deleting dataset: $dataset"
                    bq rm -r -f "${PROJECT_ID}:${dataset}" || warn "Failed to delete $dataset"
                done
            else
                warn "Skipped BigQuery dataset deletion"
            fi
        else
            info "No datasets found matching pattern 'product_reviews_*'"
        fi
    fi
}

# Function to clean up local files
cleanup_local_files() {
    info "Cleaning up local files..."
    
    local function_dir="${SCRIPT_DIR}/../function"
    local samples_dir="${SCRIPT_DIR}/../samples"
    
    # Remove function directory
    if [[ -d "$function_dir" ]]; then
        info "Removing function directory: $function_dir"
        rm -rf "$function_dir"
        log "Function directory removed"
    else
        info "Function directory not found, skipping"
    fi
    
    # Remove samples directory
    if [[ -d "$samples_dir" ]]; then
        info "Removing samples directory: $samples_dir"
        rm -rf "$samples_dir"
        log "Samples directory removed"
    else
        info "Samples directory not found, skipping"
    fi
    
    # Remove deployment info file
    if [[ -f "$DEPLOYMENT_INFO" ]]; then
        info "Removing deployment info file"
        rm -f "$DEPLOYMENT_INFO"
        log "Deployment info file removed"
    fi
    
    # Clean up old log files (keep current session logs)
    find "$SCRIPT_DIR" -name "*.log" -type f -mtime +7 -delete 2>/dev/null || true
    
    log "Local file cleanup completed"
}

# Function to verify cleanup
verify_cleanup() {
    info "Verifying cleanup..."
    
    local cleanup_issues=0
    
    # Check Cloud Function
    if [[ -n "${FUNCTION_NAME:-}" ]]; then
        if gcloud functions describe "${FUNCTION_NAME}" \
            --region="${REGION}" &>/dev/null; then
            warn "Cloud Function $FUNCTION_NAME still exists"
            ((cleanup_issues++))
        else
            log "Cloud Function cleanup verified"
        fi
    fi
    
    # Check BigQuery dataset
    if [[ -n "${DATASET_NAME:-}" ]]; then
        if bq ls -d "${PROJECT_ID}:${DATASET_NAME}" &>/dev/null; then
            warn "BigQuery dataset $DATASET_NAME still exists"
            ((cleanup_issues++))
        else
            log "BigQuery dataset cleanup verified"
        fi
    fi
    
    # Check local directories
    local function_dir="${SCRIPT_DIR}/../function"
    local samples_dir="${SCRIPT_DIR}/../samples"
    
    if [[ -d "$function_dir" ]]; then
        warn "Function directory still exists: $function_dir"
        ((cleanup_issues++))
    fi
    
    if [[ -d "$samples_dir" ]]; then
        warn "Samples directory still exists: $samples_dir"
        ((cleanup_issues++))
    fi
    
    if [[ $cleanup_issues -eq 0 ]]; then
        log "Cleanup verification completed successfully"
        return 0
    else
        warn "Cleanup verification found $cleanup_issues issue(s)"
        return 1
    fi
}

# Function to display completion information
display_completion_info() {
    log "============================================"
    if verify_cleanup; then
        log "Cleanup completed successfully!"
        log "All resources have been removed."
    else
        warn "Cleanup completed with some issues!"
        warn "Please check the warnings above and manually remove any remaining resources."
    fi
    log "============================================"
    log ""
    log "Resources that were deleted:"
    log "  - Cloud Function: ${FUNCTION_NAME:-'pattern: review-analysis-*'}"
    log "  - BigQuery Dataset: ${DATASET_NAME:-'pattern: product_reviews_*'}"
    log "  - Local function code and sample data"
    log ""
    log "Note: Google Cloud APIs remain enabled. You may disable them manually if needed:"
    log "  - cloudfunctions.googleapis.com"
    log "  - translate.googleapis.com"
    log "  - language.googleapis.com"
    log "  - bigquery.googleapis.com"
    log "  - cloudbuild.googleapis.com"
    log ""
    log "Cleanup logs saved to: $LOG_FILE"
    
    if [[ -f "$ERROR_LOG" ]] && [[ -s "$ERROR_LOG" ]]; then
        warn "Some errors occurred during cleanup. Check: $ERROR_LOG"
    fi
}

# Function to handle cleanup errors gracefully
handle_cleanup_error() {
    local exit_code=$?
    error "Cleanup failed at line $LINENO with exit code $exit_code"
    
    warn "Partial cleanup may have occurred. Please check:"
    warn "  - Cloud Functions in region: ${REGION:-us-central1}"
    warn "  - BigQuery datasets in project: ${PROJECT_ID:-unknown}"
    warn "  - Local directories: ${SCRIPT_DIR}/../function and ${SCRIPT_DIR}/../samples"
    
    display_completion_info
    exit $exit_code
}

# Function to show usage
show_usage() {
    echo "Usage: $0 [OPTIONS]"
    echo ""
    echo "Options:"
    echo "  --force              Skip confirmation prompts"
    echo "  --project PROJECT_ID Override project ID"
    echo "  --region REGION      Override region (default: us-central1)"
    echo "  --help               Show this help message"
    echo ""
    echo "Environment Variables:"
    echo "  FORCE_DESTROY=true   Same as --force flag"
    echo ""
    echo "Examples:"
    echo "  $0                           # Interactive cleanup"
    echo "  $0 --force                   # Skip confirmations"
    echo "  $0 --project my-project      # Override project"
    echo "  FORCE_DESTROY=true $0        # Skip confirmations via env var"
}

# Parse command line arguments
parse_arguments() {
    while [[ $# -gt 0 ]]; do
        case $1 in
            --force)
                export FORCE_DESTROY="true"
                shift
                ;;
            --project)
                export PROJECT_ID="$2"
                shift 2
                ;;
            --region)
                export REGION="$2"
                shift 2
                ;;
            --help)
                show_usage
                exit 0
                ;;
            *)
                error "Unknown argument: $1"
                show_usage
                exit 1
                ;;
        esac
    done
}

# Main cleanup function
main() {
    info "Starting cleanup of Smart Product Review Analysis resources..."
    
    # Initialize log files
    : > "$LOG_FILE"
    : > "$ERROR_LOG"
    
    # Parse command line arguments
    parse_arguments "$@"
    
    # Run cleanup steps
    load_deployment_info
    check_prerequisites
    confirm_destruction
    set_project_context
    delete_cloud_function
    delete_bigquery_resources
    cleanup_local_files
    display_completion_info
    
    log "Cleanup completed!"
}

# Error handling
trap 'handle_cleanup_error' ERR

# Run main function with all arguments
main "$@"