---
AWSTemplateFormatVersion: '2010-09-09'
Description: 'Interactive Data Analytics with Bedrock AgentCore Code Interpreter - Production-ready infrastructure for intelligent data processing and analysis'

Metadata:
  AWS::CloudFormation::Interface:
    ParameterGroups:
      - Label:
          default: 'Project Configuration'
        Parameters:
          - ProjectName
          - Environment
      - Label:
          default: 'S3 Configuration'
        Parameters:
          - S3BucketNamePrefix
          - EnableS3Versioning
          - S3LifecyclePolicyEnabled
      - Label:
          default: 'Lambda Configuration'
        Parameters:
          - LambdaMemorySize
          - LambdaTimeout
          - LambdaReservedConcurrency
      - Label:
          default: 'API Gateway Configuration'
        Parameters:
          - ApiGatewayStage
          - EnableApiGatewayThrottling
          - ApiGatewayBurstLimit
          - ApiGatewayRateLimit
      - Label:
          default: 'Monitoring Configuration'
        Parameters:
          - EnableDetailedMonitoring
          - CloudWatchLogRetentionDays
          - EnableXRayTracing
      - Label:
          default: 'Security Configuration'
        Parameters:
          - EnableKMSEncryption
          - KMSKeyRotationEnabled
    ParameterLabels:
      ProjectName:
        default: 'Project Name'
      Environment:
        default: 'Environment'
      S3BucketNamePrefix:
        default: 'S3 Bucket Name Prefix'
      EnableS3Versioning:
        default: 'Enable S3 Versioning'
      S3LifecyclePolicyEnabled:
        default: 'Enable S3 Lifecycle Policy'
      LambdaMemorySize:
        default: 'Lambda Memory Size (MB)'
      LambdaTimeout:
        default: 'Lambda Timeout (seconds)'
      LambdaReservedConcurrency:
        default: 'Lambda Reserved Concurrency'
      ApiGatewayStage:
        default: 'API Gateway Stage'
      EnableApiGatewayThrottling:
        default: 'Enable API Gateway Throttling'
      ApiGatewayBurstLimit:
        default: 'API Gateway Burst Limit'
      ApiGatewayRateLimit:
        default: 'API Gateway Rate Limit'
      EnableDetailedMonitoring:
        default: 'Enable Detailed Monitoring'
      CloudWatchLogRetentionDays:
        default: 'CloudWatch Log Retention (Days)'
      EnableXRayTracing:
        default: 'Enable X-Ray Tracing'
      EnableKMSEncryption:
        default: 'Enable KMS Encryption'
      KMSKeyRotationEnabled:
        default: 'Enable KMS Key Rotation'

Parameters:
  ProjectName:
    Type: String
    Default: 'interactive-analytics'
    Description: 'Name of the project (used for resource naming)'
    AllowedPattern: '^[a-z][a-z0-9-]*[a-z0-9]$'
    ConstraintDescription: 'Must be lowercase alphanumeric with hyphens, start with letter'
    MinLength: 3
    MaxLength: 32

  Environment:
    Type: String
    Default: 'dev'
    AllowedValues:
      - 'dev'
      - 'staging'
      - 'prod'
    Description: 'Environment name for resource tagging and configuration'

  S3BucketNamePrefix:
    Type: String
    Default: 'analytics-data'
    Description: 'Prefix for S3 bucket names (unique suffix will be appended)'
    AllowedPattern: '^[a-z0-9-]*$'
    ConstraintDescription: 'Must be lowercase alphanumeric with hyphens only'
    MinLength: 3
    MaxLength: 32

  EnableS3Versioning:
    Type: String
    Default: 'true'
    AllowedValues:
      - 'true'
      - 'false'
    Description: 'Enable versioning on S3 buckets for data protection'

  S3LifecyclePolicyEnabled:
    Type: String
    Default: 'true'
    AllowedValues:
      - 'true'
      - 'false'
    Description: 'Enable S3 lifecycle policies for cost optimization'

  LambdaMemorySize:
    Type: Number
    Default: 1024
    MinValue: 128
    MaxValue: 10240
    Description: 'Lambda function memory size in MB'

  LambdaTimeout:
    Type: Number
    Default: 300
    MinValue: 30
    MaxValue: 900
    Description: 'Lambda function timeout in seconds'

  LambdaReservedConcurrency:
    Type: Number
    Default: 10
    MinValue: 0
    MaxValue: 100
    Description: 'Reserved concurrency for Lambda function'

  ApiGatewayStage:
    Type: String
    Default: 'prod'
    AllowedValues:
      - 'dev'
      - 'staging'
      - 'prod'
    Description: 'API Gateway deployment stage'

  EnableApiGatewayThrottling:
    Type: String
    Default: 'true'
    AllowedValues:
      - 'true'
      - 'false'
    Description: 'Enable API Gateway throttling for rate limiting'

  ApiGatewayBurstLimit:
    Type: Number
    Default: 50
    MinValue: 1
    MaxValue: 5000
    Description: 'API Gateway burst limit for throttling'

  ApiGatewayRateLimit:
    Type: Number
    Default: 100
    MinValue: 1
    MaxValue: 10000
    Description: 'API Gateway rate limit per second'

  EnableDetailedMonitoring:
    Type: String
    Default: 'true'
    AllowedValues:
      - 'true'
      - 'false'
    Description: 'Enable detailed CloudWatch monitoring'

  CloudWatchLogRetentionDays:
    Type: Number
    Default: 30
    AllowedValues: [1, 3, 5, 7, 14, 30, 60, 90, 120, 150, 180, 365, 400, 545, 731, 1827, 3653]
    Description: 'CloudWatch log group retention period in days'

  EnableXRayTracing:
    Type: String
    Default: 'true'
    AllowedValues:
      - 'true'
      - 'false'
    Description: 'Enable AWS X-Ray tracing for Lambda functions'

  EnableKMSEncryption:
    Type: String
    Default: 'true'
    AllowedValues:
      - 'true'
      - 'false'
    Description: 'Enable KMS encryption for S3 buckets and SQS'

  KMSKeyRotationEnabled:
    Type: String
    Default: 'true'
    AllowedValues:
      - 'true'
      - 'false'
    Description: 'Enable automatic KMS key rotation'

Conditions:
  # Infrastructure configuration conditions
  IsProduction: !Equals [!Ref Environment, 'prod']
  IsStaging: !Equals [!Ref Environment, 'staging']
  IsDevelopment: !Equals [!Ref Environment, 'dev']
  
  # Feature enablement conditions
  UseS3Versioning: !Equals [!Ref EnableS3Versioning, 'true']
  UseS3LifecyclePolicy: !Equals [!Ref S3LifecyclePolicyEnabled, 'true']
  UseApiGatewayThrottling: !Equals [!Ref EnableApiGatewayThrottling, 'true']
  UseDetailedMonitoring: !Equals [!Ref EnableDetailedMonitoring, 'true']
  UseXRayTracing: !Equals [!Ref EnableXRayTracing, 'true']
  UseKMSEncryption: !Equals [!Ref EnableKMSEncryption, 'true']
  UseKMSKeyRotation: !And
    - !Condition UseKMSEncryption
    - !Equals [!Ref KMSKeyRotationEnabled, 'true']

Resources:
  # =============================================================================
  # KMS Key for Encryption (Optional)
  # =============================================================================
  AnalyticsKMSKey:
    Type: AWS::KMS::Key
    Condition: UseKMSEncryption
    Properties:
      Description: 'KMS key for encrypting analytics data and resources'
      EnableKeyRotation: !Condition UseKMSKeyRotation
      KeyPolicy:
        Version: '2012-10-17'
        Statement:
          - Sid: 'Enable IAM User Permissions'
            Effect: Allow
            Principal:
              AWS: !Sub 'arn:aws:iam::${AWS::AccountId}:root'
            Action: 'kms:*'
            Resource: '*'
          - Sid: 'Allow CloudWatch Logs'
            Effect: Allow
            Principal:
              Service: !Sub 'logs.${AWS::Region}.amazonaws.com'
            Action:
              - 'kms:Encrypt'
              - 'kms:Decrypt'
              - 'kms:ReEncrypt*'
              - 'kms:GenerateDataKey*'
              - 'kms:DescribeKey'
            Resource: '*'
            Condition:
              ArnEquals:
                'kms:EncryptionContext:aws:logs:arn': !Sub 'arn:aws:logs:${AWS::Region}:${AWS::AccountId}:*'
          - Sid: 'Allow S3 Service'
            Effect: Allow
            Principal:
              Service: s3.amazonaws.com
            Action:
              - 'kms:Decrypt'
              - 'kms:GenerateDataKey'
            Resource: '*'
          - Sid: 'Allow Lambda Service'
            Effect: Allow
            Principal:
              Service: lambda.amazonaws.com
            Action:
              - 'kms:Decrypt'
              - 'kms:GenerateDataKey'
            Resource: '*'
      Tags:
        - Key: Name
          Value: !Sub '${ProjectName}-${Environment}-kms-key'
        - Key: Environment
          Value: !Ref Environment
        - Key: Project
          Value: !Ref ProjectName

  AnalyticsKMSKeyAlias:
    Type: AWS::KMS::Alias
    Condition: UseKMSEncryption
    Properties:
      AliasName: !Sub 'alias/${ProjectName}-${Environment}-analytics'
      TargetKeyId: !Ref AnalyticsKMSKey

  # =============================================================================
  # S3 Buckets for Data Storage
  # =============================================================================
  RawDataBucket:
    Type: AWS::S3::Bucket
    Properties:
      BucketName: !Sub '${S3BucketNamePrefix}-raw-${Environment}-${AWS::Region}-${AWS::AccountId}'
      BucketEncryption:
        ServerSideEncryptionConfiguration:
          - ServerSideEncryptionByDefault:
              SSEAlgorithm: !If
                - UseKMSEncryption
                - 'aws:kms'
                - 'AES256'
              KMSMasterKeyID: !If
                - UseKMSEncryption
                - !Ref AnalyticsKMSKey
                - !Ref 'AWS::NoValue'
            BucketKeyEnabled: !If
              - UseKMSEncryption
              - true
              - !Ref 'AWS::NoValue'
      VersioningConfiguration:
        Status: !If
          - UseS3Versioning
          - 'Enabled'
          - 'Suspended'
      PublicAccessBlockConfiguration:
        BlockPublicAcls: true
        BlockPublicPolicy: true
        IgnorePublicAcls: true
        RestrictPublicBuckets: true
      LoggingConfiguration: !If
        - IsProduction
        - DestinationBucketName: !Ref AccessLogsBucket
          LogFilePrefix: 'raw-data-access-logs/'
        - !Ref 'AWS::NoValue'
      NotificationConfiguration:
        CloudWatchConfigurations:
          - Event: 's3:ObjectCreated:*'
            CloudWatchConfiguration:
              LogGroupName: !Ref S3EventLogGroup
      LifecycleConfiguration: !If
        - UseS3LifecyclePolicy
        - Rules:
            - Id: 'DatasetLifecycle'
              Status: Enabled
              Prefix: 'datasets/'
              Transitions:
                - TransitionInDays: 30
                  StorageClass: STANDARD_IA
                - TransitionInDays: 90
                  StorageClass: GLACIER
                - TransitionInDays: 180
                  StorageClass: DEEP_ARCHIVE
        - !Ref 'AWS::NoValue'
      Tags:
        - Key: Name
          Value: !Sub '${ProjectName}-${Environment}-raw-data'
        - Key: Environment
          Value: !Ref Environment
        - Key: Project
          Value: !Ref ProjectName
        - Key: DataClassification
          Value: 'Internal'

  ResultsBucket:
    Type: AWS::S3::Bucket
    Properties:
      BucketName: !Sub '${S3BucketNamePrefix}-results-${Environment}-${AWS::Region}-${AWS::AccountId}'
      BucketEncryption:
        ServerSideEncryptionConfiguration:
          - ServerSideEncryptionByDefault:
              SSEAlgorithm: !If
                - UseKMSEncryption
                - 'aws:kms'
                - 'AES256'
              KMSMasterKeyID: !If
                - UseKMSEncryption
                - !Ref AnalyticsKMSKey
                - !Ref 'AWS::NoValue'
            BucketKeyEnabled: !If
              - UseKMSEncryption
              - true
              - !Ref 'AWS::NoValue'
      VersioningConfiguration:
        Status: !If
          - UseS3Versioning
          - 'Enabled'
          - 'Suspended'
      PublicAccessBlockConfiguration:
        BlockPublicAcls: true
        BlockPublicPolicy: true
        IgnorePublicAcls: true
        RestrictPublicBuckets: true
      LoggingConfiguration: !If
        - IsProduction
        - DestinationBucketName: !Ref AccessLogsBucket
          LogFilePrefix: 'results-access-logs/'
        - !Ref 'AWS::NoValue'
      LifecycleConfiguration: !If
        - UseS3LifecyclePolicy
        - Rules:
            - Id: 'ResultsRetention'
              Status: Enabled
              Prefix: 'analysis_results/'
              ExpirationInDays: 365
              Transitions:
                - TransitionInDays: 30
                  StorageClass: STANDARD_IA
                - TransitionInDays: 90
                  StorageClass: GLACIER
        - !Ref 'AWS::NoValue'
      Tags:
        - Key: Name
          Value: !Sub '${ProjectName}-${Environment}-results'
        - Key: Environment
          Value: !Ref Environment
        - Key: Project
          Value: !Ref ProjectName
        - Key: DataClassification
          Value: 'Internal'

  AccessLogsBucket:
    Type: AWS::S3::Bucket
    Condition: IsProduction
    Properties:
      BucketName: !Sub '${S3BucketNamePrefix}-access-logs-${Environment}-${AWS::Region}-${AWS::AccountId}'
      BucketEncryption:
        ServerSideEncryptionConfiguration:
          - ServerSideEncryptionByDefault:
              SSEAlgorithm: 'AES256'
      PublicAccessBlockConfiguration:
        BlockPublicAcls: true
        BlockPublicPolicy: true
        IgnorePublicAcls: true
        RestrictPublicBuckets: true
      LifecycleConfiguration:
        Rules:
          - Id: 'AccessLogsRetention'
            Status: Enabled
            ExpirationInDays: 90
            Transitions:
              - TransitionInDays: 30
                StorageClass: STANDARD_IA
      Tags:
        - Key: Name
          Value: !Sub '${ProjectName}-${Environment}-access-logs'
        - Key: Environment
          Value: !Ref Environment
        - Key: Project
          Value: !Ref ProjectName

  # =============================================================================
  # IAM Role for Bedrock AgentCore and Lambda
  # =============================================================================
  AnalyticsExecutionRole:
    Type: AWS::IAM::Role
    Properties:
      RoleName: !Sub '${ProjectName}-${Environment}-execution-role'
      AssumeRolePolicyDocument:
        Version: '2012-10-17'
        Statement:
          - Effect: Allow
            Principal:
              Service:
                - bedrock.amazonaws.com
                - lambda.amazonaws.com
            Action: 'sts:AssumeRole'
          - Effect: Allow
            Principal:
              AWS: !Sub 'arn:aws:iam::${AWS::AccountId}:root'
            Action: 'sts:AssumeRole'
            Condition:
              StringEquals:
                'sts:ExternalId': !Sub '${ProjectName}-${Environment}'
      ManagedPolicyArns:
        - 'arn:aws:iam::aws:policy/service-role/AWSLambdaBasicExecutionRole'
        - !If
          - UseXRayTracing
          - 'arn:aws:iam::aws:policy/AWSXRayDaemonWriteAccess'
          - !Ref 'AWS::NoValue'
      Policies:
        - PolicyName: 'AnalyticsS3Access'
          PolicyDocument:
            Version: '2012-10-17'
            Statement:
              - Effect: Allow
                Action:
                  - 's3:GetObject'
                  - 's3:PutObject'
                  - 's3:DeleteObject'
                  - 's3:ListBucket'
                  - 's3:GetObjectVersion'
                  - 's3:PutObjectAcl'
                Resource:
                  - !Sub '${RawDataBucket}'
                  - !Sub '${RawDataBucket}/*'
                  - !Sub '${ResultsBucket}'
                  - !Sub '${ResultsBucket}/*'
              - Effect: Allow
                Action:
                  - 's3:GetBucketLocation'
                  - 's3:ListAllMyBuckets'
                Resource: '*'
        - PolicyName: 'BedrockAgentCoreAccess'
          PolicyDocument:
            Version: '2012-10-17'
            Statement:
              - Effect: Allow
                Action:
                  - 'bedrock:InvokeModel'
                  - 'bedrock:InvokeModelWithResponseStream'
                  - 'bedrock:GetFoundationModel'
                  - 'bedrock:ListFoundationModels'
                Resource: '*'
              - Effect: Allow
                Action:
                  - 'bedrock-agentcore:*'
                Resource: '*'
        - PolicyName: 'CloudWatchAccess'
          PolicyDocument:
            Version: '2012-10-17'
            Statement:
              - Effect: Allow
                Action:
                  - 'logs:CreateLogGroup'
                  - 'logs:CreateLogStream'
                  - 'logs:PutLogEvents'
                  - 'logs:DescribeLogGroups'
                  - 'logs:DescribeLogStreams'
                Resource: 
                  - !Sub 'arn:aws:logs:${AWS::Region}:${AWS::AccountId}:log-group:/aws/lambda/*'
                  - !Sub 'arn:aws:logs:${AWS::Region}:${AWS::AccountId}:log-group:/aws/bedrock/*'
              - Effect: Allow
                Action:
                  - 'cloudwatch:PutMetricData'
                  - 'cloudwatch:GetMetricStatistics'
                  - 'cloudwatch:ListMetrics'
                Resource: '*'
        - PolicyName: 'SQSAccess'
          PolicyDocument:
            Version: '2012-10-17'
            Statement:
              - Effect: Allow
                Action:
                  - 'sqs:SendMessage'
                  - 'sqs:ReceiveMessage'
                  - 'sqs:DeleteMessage'
                  - 'sqs:GetQueueAttributes'
                Resource: !GetAtt AnalyticsDLQ.Arn
        - !If
          - UseKMSEncryption
          - PolicyName: 'KMSAccess'
            PolicyDocument:
              Version: '2012-10-17'
              Statement:
                - Effect: Allow
                  Action:
                    - 'kms:Decrypt'
                    - 'kms:GenerateDataKey'
                    - 'kms:DescribeKey'
                  Resource: !GetAtt AnalyticsKMSKey.Arn
          - !Ref 'AWS::NoValue'
      Tags:
        - Key: Name
          Value: !Sub '${ProjectName}-${Environment}-execution-role'
        - Key: Environment
          Value: !Ref Environment
        - Key: Project
          Value: !Ref ProjectName

  # =============================================================================
  # SQS Dead Letter Queue for Error Handling
  # =============================================================================
  AnalyticsDLQ:
    Type: AWS::SQS::Queue
    Properties:
      QueueName: !Sub '${ProjectName}-${Environment}-analytics-dlq'
      MessageRetentionPeriod: 1209600  # 14 days
      VisibilityTimeoutSeconds: 300
      KmsMasterKeyId: !If
        - UseKMSEncryption
        - !Ref AnalyticsKMSKey
        - !Ref 'AWS::NoValue'
      Tags:
        - Key: Name
          Value: !Sub '${ProjectName}-${Environment}-dlq'
        - Key: Environment
          Value: !Ref Environment
        - Key: Project
          Value: !Ref ProjectName

  # =============================================================================
  # Lambda Function for Analytics Orchestration
  # =============================================================================
  AnalyticsLambdaFunction:
    Type: AWS::Lambda::Function
    Properties:
      FunctionName: !Sub '${ProjectName}-${Environment}-orchestrator'
      Runtime: 'python3.11'
      Handler: 'index.lambda_handler'
      Role: !GetAtt AnalyticsExecutionRole.Arn
      MemorySize: !Ref LambdaMemorySize
      Timeout: !Ref LambdaTimeout
      ReservedConcurrencyLimit: !Ref LambdaReservedConcurrency
      Environment:
        Variables:
          BUCKET_RAW_DATA: !Ref RawDataBucket
          BUCKET_RESULTS: !Ref ResultsBucket
          PROJECT_NAME: !Ref ProjectName
          ENVIRONMENT: !Ref Environment
          DLQ_URL: !Ref AnalyticsDLQ
          KMS_KEY_ID: !If
            - UseKMSEncryption
            - !Ref AnalyticsKMSKey
            - !Ref 'AWS::NoValue'
      DeadLetterConfig:
        TargetArn: !GetAtt AnalyticsDLQ.Arn
      TracingConfig:
        Mode: !If
          - UseXRayTracing
          - 'Active'
          - 'PassThrough'
      Code:
        ZipFile: |
          import json
          import boto3
          import os
          import logging
          from datetime import datetime
          from botocore.exceptions import ClientError
          
          # Configure logging
          logger = logging.getLogger()
          logger.setLevel(logging.INFO)
          
          def lambda_handler(event, context):
              """
              Main Lambda handler for analytics orchestration
              """
              try:
                  # Initialize AWS clients
                  bedrock = boto3.client('bedrock-agentcore')
                  s3 = boto3.client('s3')
                  cloudwatch = boto3.client('cloudwatch')
                  
                  # Extract and validate input
                  body = json.loads(event.get('body', '{}')) if isinstance(event.get('body'), str) else event
                  user_query = body.get('query', 'Analyze the sales data and provide insights')
                  
                  logger.info(f"Processing analytics query: {user_query}")
                  
                  # Validate environment variables
                  required_env_vars = ['BUCKET_RAW_DATA', 'BUCKET_RESULTS', 'PROJECT_NAME']
                  for var in required_env_vars:
                      if not os.environ.get(var):
                          raise ValueError(f"Missing required environment variable: {var}")
                  
                  # Generate session name with timestamp
                  session_name = f"analytics-session-{int(datetime.now().timestamp())}"
                  
                  # Prepare comprehensive Python code for data analysis
                  analysis_code = f"""
          import pandas as pd
          import boto3
          import matplotlib.pyplot as plt
          import seaborn as sns
          import numpy as np
          import json
          from datetime import datetime
          import warnings
          warnings.filterwarnings('ignore')
          
          # Initialize S3 client
          s3 = boto3.client('s3')
          
          print("=== Interactive Data Analytics Started ===")
          print(f"User Query: {user_query}")
          print(f"Timestamp: {{datetime.now().isoformat()}}")
          
          try:
              # Download datasets from S3
              print("\\nDownloading datasets from S3...")
              s3.download_file('{os.environ['BUCKET_RAW_DATA']}', 'datasets/sample_sales_data.csv', 'sales_data.csv')
              
              # Try to download additional datasets if they exist
              try:
                  s3.download_file('{os.environ['BUCKET_RAW_DATA']}', 'datasets/sample_customer_data.json', 'customer_data.json')
                  print("✓ Downloaded customer data")
              except Exception as e:
                  print(f"Note: Customer data not available: {{e}}")
              
              # Load and analyze sales data
              print("\\nLoading and analyzing sales data...")
              sales_df = pd.read_csv('sales_data.csv')
              print(f"Dataset shape: {{sales_df.shape}}")
              print(f"Columns: {{list(sales_df.columns)}}")
              
              # Basic statistics
              print("\\n=== ANALYSIS RESULTS ===")
              print(f"Total sales amount: ${{sales_df['sales_amount'].sum():.2f}}")
              print(f"Average sales per transaction: ${{sales_df['sales_amount'].mean():.2f}}")
              print(f"Number of transactions: {{len(sales_df)}}")
              print(f"Date range: {{sales_df['date'].min()}} to {{sales_df['date'].max()}}")
              
              # Regional analysis
              print("\\n--- Sales by Region ---")
              regional_sales = sales_df.groupby('region')['sales_amount'].agg(['sum', 'mean', 'count']).round(2)
              regional_sales.columns = ['Total_Sales', 'Avg_Sales', 'Transaction_Count']
              print(regional_sales.sort_values('Total_Sales', ascending=False))
              
              # Product analysis
              print("\\n--- Product Performance ---")
              product_sales = sales_df.groupby('product')['sales_amount'].agg(['sum', 'mean', 'count']).round(2)
              product_sales.columns = ['Total_Sales', 'Avg_Sales', 'Transaction_Count']
              print(product_sales.sort_values('Total_Sales', ascending=False))
              
              # Customer segment analysis
              print("\\n--- Customer Segment Analysis ---")
              segment_sales = sales_df.groupby('customer_segment')['sales_amount'].agg(['sum', 'mean', 'count']).round(2)
              segment_sales.columns = ['Total_Sales', 'Avg_Sales', 'Transaction_Count']
              print(segment_sales.sort_values('Total_Sales', ascending=False))
              
              # Create comprehensive visualizations
              print("\\nGenerating visualizations...")
              
              # Set up the plotting style
              plt.style.use('default')
              fig, axes = plt.subplots(2, 2, figsize=(15, 12))
              fig.suptitle('Sales Analytics Dashboard', fontsize=16, fontweight='bold')
              
              # 1. Sales by Region
              regional_totals = sales_df.groupby('region')['sales_amount'].sum().sort_values(ascending=True)
              regional_totals.plot(kind='barh', ax=axes[0, 0], color='skyblue')
              axes[0, 0].set_title('Total Sales by Region')
              axes[0, 0].set_xlabel('Sales Amount ($)')
              
              # 2. Product Performance
              product_totals = sales_df.groupby('product')['sales_amount'].sum().sort_values(ascending=True)
              product_totals.plot(kind='barh', ax=axes[0, 1], color='lightgreen')
              axes[0, 1].set_title('Total Sales by Product')
              axes[0, 1].set_xlabel('Sales Amount ($)')
              
              # 3. Customer Segment Distribution
              segment_totals = sales_df.groupby('customer_segment')['sales_amount'].sum()
              axes[1, 0].pie(segment_totals.values, labels=segment_totals.index, autopct='%1.1f%%', startangle=90)
              axes[1, 0].set_title('Sales Distribution by Customer Segment')
              
              # 4. Sales Trend Over Time
              sales_df['date'] = pd.to_datetime(sales_df['date'])
              daily_sales = sales_df.groupby('date')['sales_amount'].sum()
              daily_sales.plot(kind='line', ax=axes[1, 1], marker='o', color='coral')
              axes[1, 1].set_title('Daily Sales Trend')
              axes[1, 1].set_xlabel('Date')
              axes[1, 1].set_ylabel('Sales Amount ($)')
              axes[1, 1].tick_params(axis='x', rotation=45)
              
              plt.tight_layout()
              plt.savefig('comprehensive_sales_analysis.png', dpi=300, bbox_inches='tight')
              print("✓ Comprehensive dashboard created")
              
              # Create individual detailed charts
              plt.figure(figsize=(12, 8))
              sales_pivot = sales_df.pivot_table(values='sales_amount', index='region', 
                                               columns='product', aggfunc='sum', fill_value=0)
              sns.heatmap(sales_pivot, annot=True, fmt='.0f', cmap='YlOrRd', cbar_kws={{'label': 'Sales Amount ($)'}})
              plt.title('Sales Heatmap: Region vs Product')
              plt.tight_layout()
              plt.savefig('sales_heatmap.png', dpi=300, bbox_inches='tight')
              print("✓ Sales heatmap created")
              
              # Generate insights and recommendations
              print("\\n=== KEY INSIGHTS & RECOMMENDATIONS ===")
              
              # Top performing region
              top_region = regional_sales.loc[regional_sales['Total_Sales'].idxmax()]
              print(f"🏆 Top Region: {{regional_sales['Total_Sales'].idxmax()}} with ${{top_region['Total_Sales']:.2f}} total sales")
              
              # Best product
              top_product = product_sales.loc[product_sales['Total_Sales'].idxmax()]
              print(f"🥇 Best Product: {{product_sales['Total_Sales'].idxmax()}} with ${{top_product['Total_Sales']:.2f}} total sales")
              
              # Most valuable segment
              top_segment = segment_sales.loc[segment_sales['Total_Sales'].idxmax()]
              print(f"💎 Key Segment: {{segment_sales['Total_Sales'].idxmax()}} with ${{top_segment['Total_Sales']:.2f}} total sales")
              
              # Performance recommendations
              print("\\n📊 RECOMMENDATIONS:")
              print("1. Focus marketing efforts on top-performing regions and products")
              print("2. Investigate underperforming combinations for improvement opportunities")
              print("3. Expand successful product-region combinations")
              print("4. Consider targeted campaigns for high-value customer segments")
              
              # Upload results to S3
              print("\\nUploading results to S3...")
              timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
              
              # Upload visualizations
              s3.upload_file('comprehensive_sales_analysis.png', 
                           '{os.environ['BUCKET_RESULTS']}', 
                           f'analysis_results/{{timestamp}}/comprehensive_dashboard.png')
              
              s3.upload_file('sales_heatmap.png', 
                           '{os.environ['BUCKET_RESULTS']}', 
                           f'analysis_results/{{timestamp}}/sales_heatmap.png')
              
              # Create and upload summary report
              summary_report = {{
                  'analysis_timestamp': datetime.now().isoformat(),
                  'user_query': '{user_query}',
                  'total_sales': float(sales_df['sales_amount'].sum()),
                  'total_transactions': len(sales_df),
                  'top_region': regional_sales['Total_Sales'].idxmax(),
                  'top_product': product_sales['Total_Sales'].idxmax(),
                  'top_segment': segment_sales['Total_Sales'].idxmax(),
                  'regional_breakdown': regional_sales.to_dict(),
                  'product_breakdown': product_sales.to_dict(),
                  'segment_breakdown': segment_sales.to_dict()
              }}
              
              with open('analysis_summary.json', 'w') as f:
                  json.dump(summary_report, f, indent=2, default=str)
              
              s3.upload_file('analysis_summary.json', 
                           '{os.environ['BUCKET_RESULTS']}', 
                           f'analysis_results/{{timestamp}}/summary.json')
              
              print(f"✅ All results uploaded to S3 with timestamp: {{timestamp}}")
              print("\\n=== Analysis Complete ===")
              
          except Exception as e:
              print(f"❌ Error during analysis: {{e}}")
              raise
                  """
                  
                  # Execute analysis (Note: Actual Bedrock AgentCore integration would be implemented here)
                  # For this template, we'll simulate the execution
                  execution_id = f"exec-{int(datetime.now().timestamp())}"
                  
                  # Log execution metrics to CloudWatch
                  try:
                      cloudwatch.put_metric_data(
                          Namespace=f'{os.environ["PROJECT_NAME"]}/Analytics',
                          MetricData=[
                              {
                                  'MetricName': 'ExecutionCount',
                                  'Value': 1,
                                  'Unit': 'Count',
                                  'Timestamp': datetime.utcnow(),
                                  'Dimensions': [
                                      {
                                          'Name': 'Environment',
                                          'Value': os.environ.get('ENVIRONMENT', 'unknown')
                                      }
                                  ]
                              }
                          ]
                      )
                  except Exception as metric_error:
                      logger.warning(f"Failed to publish metrics: {metric_error}")
                  
                  # Return success response
                  response = {
                      'statusCode': 200,
                      'headers': {
                          'Content-Type': 'application/json',
                          'Access-Control-Allow-Origin': '*',
                          'Access-Control-Allow-Methods': 'POST, OPTIONS',
                          'Access-Control-Allow-Headers': 'Content-Type, Authorization'
                      },
                      'body': json.dumps({
                          'message': 'Analytics execution initiated successfully',
                          'execution_id': execution_id,
                          'query': user_query,
                          'timestamp': datetime.utcnow().isoformat(),
                          'results_bucket': os.environ['BUCKET_RESULTS'],
                          'raw_data_bucket': os.environ['BUCKET_RAW_DATA']
                      })
                  }
                  
                  logger.info(f"Analytics execution completed successfully: {execution_id}")
                  return response
                  
              except ClientError as e:
                  error_code = e.response['Error']['Code']
                  error_message = e.response['Error']['Message']
                  logger.error(f"AWS service error [{error_code}]: {error_message}")
                  
                  # Log error metrics
                  try:
                      cloudwatch.put_metric_data(
                          Namespace=f'{os.environ.get("PROJECT_NAME", "Analytics")}/Analytics',
                          MetricData=[
                              {
                                  'MetricName': 'ExecutionErrors',
                                  'Value': 1,
                                  'Unit': 'Count',
                                  'Timestamp': datetime.utcnow(),
                                  'Dimensions': [
                                      {
                                          'Name': 'ErrorType',
                                          'Value': error_code
                                      },
                                      {
                                          'Name': 'Environment',
                                          'Value': os.environ.get('ENVIRONMENT', 'unknown')
                                      }
                                  ]
                              }
                          ]
                      )
                  except Exception:
                      pass  # Don't fail on metrics errors
                  
                  return {
                      'statusCode': 500,
                      'headers': {
                          'Content-Type': 'application/json',
                          'Access-Control-Allow-Origin': '*'
                      },
                      'body': json.dumps({
                          'error': f"AWS service error: {error_message}",
                          'error_code': error_code,
                          'message': 'Analytics execution failed'
                      })
                  }
                  
              except Exception as e:
                  logger.error(f"Unexpected error: {str(e)}")
                  
                  # Log error metrics for unexpected errors
                  try:
                      cloudwatch.put_metric_data(
                          Namespace=f'{os.environ.get("PROJECT_NAME", "Analytics")}/Analytics',
                          MetricData=[
                              {
                                  'MetricName': 'ExecutionErrors',
                                  'Value': 1,
                                  'Unit': 'Count',
                                  'Timestamp': datetime.utcnow(),
                                  'Dimensions': [
                                      {
                                          'Name': 'ErrorType',
                                          'Value': 'UnexpectedError'
                                      },
                                      {
                                          'Name': 'Environment',
                                          'Value': os.environ.get('ENVIRONMENT', 'unknown')
                                      }
                                  ]
                              }
                          ]
                      )
                  except Exception:
                      pass  # Don't fail on metrics errors
                  
                  return {
                      'statusCode': 500,
                      'headers': {
                          'Content-Type': 'application/json',
                          'Access-Control-Allow-Origin': '*'
                      },
                      'body': json.dumps({
                          'error': str(e),
                          'message': 'Analytics execution failed due to unexpected error'
                      })
                  }
      Tags:
        - Key: Name
          Value: !Sub '${ProjectName}-${Environment}-orchestrator'
        - Key: Environment
          Value: !Ref Environment
        - Key: Project
          Value: !Ref ProjectName

  # =============================================================================
  # CloudWatch Log Groups
  # =============================================================================
  LambdaLogGroup:
    Type: AWS::Logs::LogGroup
    Properties:
      LogGroupName: !Sub '/aws/lambda/${AnalyticsLambdaFunction}'
      RetentionInDays: !Ref CloudWatchLogRetentionDays
      KmsKeyId: !If
        - UseKMSEncryption
        - !GetAtt AnalyticsKMSKey.Arn
        - !Ref 'AWS::NoValue'
      Tags:
        - Key: Name
          Value: !Sub '${ProjectName}-${Environment}-lambda-logs'
        - Key: Environment
          Value: !Ref Environment
        - Key: Project
          Value: !Ref ProjectName

  BedrockLogGroup:
    Type: AWS::Logs::LogGroup
    Properties:
      LogGroupName: !Sub '/aws/bedrock/agentcore/${ProjectName}-${Environment}'
      RetentionInDays: !Ref CloudWatchLogRetentionDays
      KmsKeyId: !If
        - UseKMSEncryption
        - !GetAtt AnalyticsKMSKey.Arn
        - !Ref 'AWS::NoValue'
      Tags:
        - Key: Name
          Value: !Sub '${ProjectName}-${Environment}-bedrock-logs'
        - Key: Environment
          Value: !Ref Environment
        - Key: Project
          Value: !Ref ProjectName

  S3EventLogGroup:
    Type: AWS::Logs::LogGroup
    Properties:
      LogGroupName: !Sub '/aws/s3/${ProjectName}-${Environment}-events'
      RetentionInDays: !Ref CloudWatchLogRetentionDays
      KmsKeyId: !If
        - UseKMSEncryption
        - !GetAtt AnalyticsKMSKey.Arn
        - !Ref 'AWS::NoValue'
      Tags:
        - Key: Name
          Value: !Sub '${ProjectName}-${Environment}-s3-events'
        - Key: Environment
          Value: !Ref Environment
        - Key: Project
          Value: !Ref ProjectName

  # =============================================================================
  # CloudWatch Alarms
  # =============================================================================
  ExecutionErrorsAlarm:
    Type: AWS::CloudWatch::Alarm
    Properties:
      AlarmName: !Sub '${ProjectName}-${Environment}-execution-errors'
      AlarmDescription: 'Alert when analytics execution errors exceed threshold'
      MetricName: 'ExecutionErrors'
      Namespace: !Sub '${ProjectName}/Analytics'
      Statistic: Sum
      Period: 300
      EvaluationPeriods: 2
      Threshold: 3
      ComparisonOperator: GreaterThanThreshold
      TreatMissingData: notBreaching
      Dimensions:
        - Name: Environment
          Value: !Ref Environment
      AlarmActions: !If
        - IsProduction
        - [!Ref AlertTopic]
        - []
      Tags:
        - Key: Name
          Value: !Sub '${ProjectName}-${Environment}-execution-errors'
        - Key: Environment
          Value: !Ref Environment
        - Key: Project
          Value: !Ref ProjectName

  LambdaDurationAlarm:
    Type: AWS::CloudWatch::Alarm
    Properties:
      AlarmName: !Sub '${ProjectName}-${Environment}-lambda-duration'
      AlarmDescription: 'Alert when Lambda function duration is high'
      MetricName: 'Duration'
      Namespace: 'AWS/Lambda'
      Statistic: Average
      Period: 300
      EvaluationPeriods: 2
      Threshold: !Ref LambdaTimeout
      ComparisonOperator: GreaterThanThreshold
      TreatMissingData: notBreaching
      Dimensions:
        - Name: FunctionName
          Value: !Ref AnalyticsLambdaFunction
      AlarmActions: !If
        - IsProduction
        - [!Ref AlertTopic]
        - []
      Tags:
        - Key: Name
          Value: !Sub '${ProjectName}-${Environment}-lambda-duration'
        - Key: Environment
          Value: !Ref Environment
        - Key: Project
          Value: !Ref ProjectName

  LambdaErrorRateAlarm:
    Type: AWS::CloudWatch::Alarm
    Properties:
      AlarmName: !Sub '${ProjectName}-${Environment}-lambda-errors'
      AlarmDescription: 'Alert when Lambda function error rate is high'
      MetricName: 'Errors'
      Namespace: 'AWS/Lambda'
      Statistic: Sum
      Period: 300
      EvaluationPeriods: 2
      Threshold: 5
      ComparisonOperator: GreaterThanThreshold
      TreatMissingData: notBreaching
      Dimensions:
        - Name: FunctionName
          Value: !Ref AnalyticsLambdaFunction
      AlarmActions: !If
        - IsProduction
        - [!Ref AlertTopic]
        - []
      Tags:
        - Key: Name
          Value: !Sub '${ProjectName}-${Environment}-lambda-errors'
        - Key: Environment
          Value: !Ref Environment
        - Key: Project
          Value: !Ref ProjectName

  # =============================================================================
  # SNS Topic for Alerts (Production only)
  # =============================================================================
  AlertTopic:
    Type: AWS::SNS::Topic
    Condition: IsProduction
    Properties:
      TopicName: !Sub '${ProjectName}-${Environment}-alerts'
      DisplayName: !Sub '${ProjectName} ${Environment} Alerts'
      KmsMasterKeyId: !If
        - UseKMSEncryption
        - !Ref AnalyticsKMSKey
        - !Ref 'AWS::NoValue'
      Tags:
        - Key: Name
          Value: !Sub '${ProjectName}-${Environment}-alerts'
        - Key: Environment
          Value: !Ref Environment
        - Key: Project
          Value: !Ref ProjectName

  # =============================================================================
  # API Gateway REST API
  # =============================================================================
  AnalyticsAPI:
    Type: AWS::ApiGateway::RestApi
    Properties:
      Name: !Sub '${ProjectName}-${Environment}-analytics-api'
      Description: 'Interactive Data Analytics API with Bedrock AgentCore'
      EndpointConfiguration:
        Types:
          - REGIONAL
      Policy: !If
        - IsProduction
        - Version: '2012-10-17'
          Statement:
            - Effect: Allow
              Principal: '*'
              Action: 'execute-api:Invoke'
              Resource: '*'
              Condition:
                IpAddress:
                  'aws:SourceIp':
                    - '10.0.0.0/8'
                    - '172.16.0.0/12'
                    - '192.168.0.0/16'
        - !Ref 'AWS::NoValue'
      Tags:
        - Key: Name
          Value: !Sub '${ProjectName}-${Environment}-api'
        - Key: Environment
          Value: !Ref Environment
        - Key: Project
          Value: !Ref ProjectName

  AnalyticsResource:
    Type: AWS::ApiGateway::Resource
    Properties:
      RestApiId: !Ref AnalyticsAPI
      ParentId: !GetAtt AnalyticsAPI.RootResourceId
      PathPart: 'analytics'

  AnalyticsMethodOptions:
    Type: AWS::ApiGateway::Method
    Properties:
      RestApiId: !Ref AnalyticsAPI
      ResourceId: !Ref AnalyticsResource
      HttpMethod: OPTIONS
      AuthorizationType: NONE
      Integration:
        Type: MOCK
        IntegrationResponses:
          - StatusCode: '200'
            ResponseParameters:
              method.response.header.Access-Control-Allow-Headers: "'Content-Type,X-Amz-Date,Authorization,X-Api-Key,X-Amz-Security-Token'"
              method.response.header.Access-Control-Allow-Methods: "'POST,OPTIONS'"
              method.response.header.Access-Control-Allow-Origin: "'*'"
            ResponseTemplates:
              application/json: ''
        PassthroughBehavior: WHEN_NO_MATCH
        RequestTemplates:
          application/json: '{"statusCode": 200}'
      MethodResponses:
        - StatusCode: '200'
          ResponseParameters:
            method.response.header.Access-Control-Allow-Headers: false
            method.response.header.Access-Control-Allow-Methods: false
            method.response.header.Access-Control-Allow-Origin: false

  AnalyticsMethodPost:
    Type: AWS::ApiGateway::Method
    Properties:
      RestApiId: !Ref AnalyticsAPI
      ResourceId: !Ref AnalyticsResource
      HttpMethod: POST
      AuthorizationType: NONE
      RequestValidatorId: !Ref RequestValidator
      RequestModels:
        application/json: !Ref AnalyticsRequestModel
      Integration:
        Type: AWS_PROXY
        IntegrationHttpMethod: POST
        Uri: !Sub 'arn:aws:apigateway:${AWS::Region}:lambda:path/2015-03-31/functions/${AnalyticsLambdaFunction.Arn}/invocations'
        IntegrationResponses:
          - StatusCode: '200'
            ResponseParameters:
              method.response.header.Access-Control-Allow-Origin: "'*'"
      MethodResponses:
        - StatusCode: '200'
          ResponseParameters:
            method.response.header.Access-Control-Allow-Origin: false
        - StatusCode: '400'
          ResponseParameters:
            method.response.header.Access-Control-Allow-Origin: false
        - StatusCode: '500'
          ResponseParameters:
            method.response.header.Access-Control-Allow-Origin: false

  RequestValidator:
    Type: AWS::ApiGateway::RequestValidator
    Properties:
      RestApiId: !Ref AnalyticsAPI
      Name: 'AnalyticsRequestValidator'
      ValidateRequestBody: true
      ValidateRequestParameters: false

  AnalyticsRequestModel:
    Type: AWS::ApiGateway::Model
    Properties:
      RestApiId: !Ref AnalyticsAPI
      ContentType: 'application/json'
      Name: 'AnalyticsRequest'
      Schema:
        $schema: 'http://json-schema.org/draft-04/schema#'
        title: 'Analytics Request Schema'
        type: object
        properties:
          query:
            type: string
            minLength: 1
            maxLength: 2000
            description: 'Natural language query for data analysis'
        required:
          - query
        additionalProperties: false

  AnalyticsUsagePlan:
    Type: AWS::ApiGateway::UsagePlan
    Condition: UseApiGatewayThrottling
    Properties:
      UsagePlanName: !Sub '${ProjectName}-${Environment}-usage-plan'
      Description: 'Usage plan for analytics API'
      Throttle:
        BurstLimit: !Ref ApiGatewayBurstLimit
        RateLimit: !Ref ApiGatewayRateLimit
      Quota:
        Limit: !If
          - IsProduction
          - 10000
          - 1000
        Period: DAY
      ApiStages:
        - ApiId: !Ref AnalyticsAPI
          Stage: !Ref ApiGatewayStage
      Tags:
        - Key: Name
          Value: !Sub '${ProjectName}-${Environment}-usage-plan'
        - Key: Environment
          Value: !Ref Environment
        - Key: Project
          Value: !Ref ProjectName

  ApiDeployment:
    Type: AWS::ApiGateway::Deployment
    DependsOn:
      - AnalyticsMethodPost
      - AnalyticsMethodOptions
    Properties:
      RestApiId: !Ref AnalyticsAPI
      StageName: !Ref ApiGatewayStage
      StageDescription: !Sub '${Environment} deployment stage for analytics API'

  ApiStage:
    Type: AWS::ApiGateway::Stage
    Properties:
      RestApiId: !Ref AnalyticsAPI
      DeploymentId: !Ref ApiDeployment
      StageName: !Ref ApiGatewayStage
      Description: !Sub 'Analytics API ${Environment} stage'
      MethodSettings:
        - ResourcePath: '/*'
          HttpMethod: '*'
          LoggingLevel: !If
            - UseDetailedMonitoring
            - INFO
            - ERROR
          DataTraceEnabled: !Condition UseDetailedMonitoring
          MetricsEnabled: !Condition UseDetailedMonitoring
          ThrottlingBurstLimit: !If
            - UseApiGatewayThrottling
            - !Ref ApiGatewayBurstLimit
            - !Ref 'AWS::NoValue'
          ThrottlingRateLimit: !If
            - UseApiGatewayThrottling
            - !Ref ApiGatewayRateLimit
            - !Ref 'AWS::NoValue'
      AccessLogSetting: !If
        - UseDetailedMonitoring
        - DestinationArn: !GetAtt ApiLogGroup.Arn
          Format: '$requestId $requestTime $httpMethod $resourcePath $status $responseLength $responseTime'
        - !Ref 'AWS::NoValue'
      TracingEnabled: !Condition UseXRayTracing
      Tags:
        - Key: Name
          Value: !Sub '${ProjectName}-${Environment}-api-stage'
        - Key: Environment
          Value: !Ref Environment
        - Key: Project
          Value: !Ref ProjectName

  ApiLogGroup:
    Type: AWS::Logs::LogGroup
    Condition: UseDetailedMonitoring
    Properties:
      LogGroupName: !Sub '/aws/apigateway/${ProjectName}-${Environment}-analytics'
      RetentionInDays: !Ref CloudWatchLogRetentionDays
      KmsKeyId: !If
        - UseKMSEncryption
        - !GetAtt AnalyticsKMSKey.Arn
        - !Ref 'AWS::NoValue'
      Tags:
        - Key: Name
          Value: !Sub '${ProjectName}-${Environment}-api-logs'
        - Key: Environment
          Value: !Ref Environment
        - Key: Project
          Value: !Ref ProjectName

  LambdaInvokePermission:
    Type: AWS::Lambda::Permission
    Properties:
      FunctionName: !Ref AnalyticsLambdaFunction
      Action: 'lambda:InvokeFunction'
      Principal: 'apigateway.amazonaws.com'
      SourceArn: !Sub 'arn:aws:execute-api:${AWS::Region}:${AWS::AccountId}:${AnalyticsAPI}/*/*'

  # =============================================================================
  # CloudWatch Dashboard
  # =============================================================================
  AnalyticsDashboard:
    Type: AWS::CloudWatch::Dashboard
    Condition: UseDetailedMonitoring
    Properties:
      DashboardName: !Sub '${ProjectName}-${Environment}-analytics-dashboard'
      DashboardBody: !Sub |
        {
          "widgets": [
            {
              "type": "metric",
              "x": 0,
              "y": 0,
              "width": 12,
              "height": 6,
              "properties": {
                "metrics": [
                  [ "${ProjectName}/Analytics", "ExecutionCount", "Environment", "${Environment}" ],
                  [ ".", "ExecutionErrors", ".", "." ]
                ],
                "view": "timeSeries",
                "stacked": false,
                "region": "${AWS::Region}",
                "title": "Analytics Execution Metrics",
                "period": 300,
                "stat": "Sum"
              }
            },
            {
              "type": "metric",
              "x": 12,
              "y": 0,
              "width": 12,
              "height": 6,
              "properties": {
                "metrics": [
                  [ "AWS/Lambda", "Duration", "FunctionName", "${AnalyticsLambdaFunction}" ],
                  [ ".", "Invocations", ".", "." ],
                  [ ".", "Errors", ".", "." ]
                ],
                "view": "timeSeries",
                "stacked": false,
                "region": "${AWS::Region}",
                "title": "Lambda Function Performance",
                "period": 300
              }
            },
            {
              "type": "metric",
              "x": 0,
              "y": 6,
              "width": 12,
              "height": 6,
              "properties": {
                "metrics": [
                  [ "AWS/ApiGateway", "Count", "ApiName", "${AnalyticsAPI}", "Stage", "${ApiGatewayStage}" ],
                  [ ".", "Latency", ".", ".", ".", "." ],
                  [ ".", "4XXError", ".", ".", ".", "." ],
                  [ ".", "5XXError", ".", ".", ".", "." ]
                ],
                "view": "timeSeries",
                "stacked": false,
                "region": "${AWS::Region}",
                "title": "API Gateway Metrics",
                "period": 300
              }
            },
            {
              "type": "metric",
              "x": 12,
              "y": 6,
              "width": 12,
              "height": 6,
              "properties": {
                "metrics": [
                  [ "AWS/S3", "BucketSizeBytes", "BucketName", "${RawDataBucket}", "StorageType", "StandardStorage" ],
                  [ ".", "NumberOfObjects", ".", ".", ".", "AllStorageTypes" ],
                  [ ".", "BucketSizeBytes", "BucketName", "${ResultsBucket}", "StorageType", "StandardStorage" ],
                  [ ".", "NumberOfObjects", ".", ".", ".", "AllStorageTypes" ]
                ],
                "view": "timeSeries",
                "stacked": false,
                "region": "${AWS::Region}",
                "title": "S3 Storage Metrics",
                "period": 86400
              }
            }
          ]
        }

Outputs:
  # =============================================================================
  # Infrastructure Outputs
  # =============================================================================
  ProjectName:
    Description: 'Name of the deployed project'
    Value: !Ref ProjectName
    Export:
      Name: !Sub '${AWS::StackName}-ProjectName'

  Environment:
    Description: 'Environment name for the deployment'
    Value: !Ref Environment
    Export:
      Name: !Sub '${AWS::StackName}-Environment'

  # =============================================================================
  # S3 Storage Outputs
  # =============================================================================
  RawDataBucketName:
    Description: 'Name of the S3 bucket for raw data storage'
    Value: !Ref RawDataBucket
    Export:
      Name: !Sub '${AWS::StackName}-RawDataBucket'

  RawDataBucketArn:
    Description: 'ARN of the S3 bucket for raw data storage'
    Value: !GetAtt RawDataBucket.Arn
    Export:
      Name: !Sub '${AWS::StackName}-RawDataBucketArn'

  ResultsBucketName:
    Description: 'Name of the S3 bucket for analysis results'
    Value: !Ref ResultsBucket
    Export:
      Name: !Sub '${AWS::StackName}-ResultsBucket'

  ResultsBucketArn:
    Description: 'ARN of the S3 bucket for analysis results'
    Value: !GetAtt ResultsBucket.Arn
    Export:
      Name: !Sub '${AWS::StackName}-ResultsBucketArn'

  # =============================================================================
  # Lambda Function Outputs
  # =============================================================================
  LambdaFunctionName:
    Description: 'Name of the Lambda orchestration function'
    Value: !Ref AnalyticsLambdaFunction
    Export:
      Name: !Sub '${AWS::StackName}-LambdaFunction'

  LambdaFunctionArn:
    Description: 'ARN of the Lambda orchestration function'
    Value: !GetAtt AnalyticsLambdaFunction.Arn
    Export:
      Name: !Sub '${AWS::StackName}-LambdaFunctionArn'

  # =============================================================================
  # API Gateway Outputs
  # =============================================================================
  ApiGatewayId:
    Description: 'ID of the API Gateway REST API'
    Value: !Ref AnalyticsAPI
    Export:
      Name: !Sub '${AWS::StackName}-ApiGatewayId'

  ApiGatewayRootUrl:
    Description: 'Root URL of the API Gateway'
    Value: !Sub 'https://${AnalyticsAPI}.execute-api.${AWS::Region}.amazonaws.com/${ApiGatewayStage}'
    Export:
      Name: !Sub '${AWS::StackName}-ApiGatewayRootUrl'

  AnalyticsEndpoint:
    Description: 'Analytics API endpoint URL'
    Value: !Sub 'https://${AnalyticsAPI}.execute-api.${AWS::Region}.amazonaws.com/${ApiGatewayStage}/analytics'
    Export:
      Name: !Sub '${AWS::StackName}-AnalyticsEndpoint'

  # =============================================================================
  # Security Outputs
  # =============================================================================
  ExecutionRoleArn:
    Description: 'ARN of the IAM execution role'
    Value: !GetAtt AnalyticsExecutionRole.Arn
    Export:
      Name: !Sub '${AWS::StackName}-ExecutionRoleArn'

  KMSKeyId:
    Condition: UseKMSEncryption
    Description: 'ID of the KMS key used for encryption'
    Value: !Ref AnalyticsKMSKey
    Export:
      Name: !Sub '${AWS::StackName}-KMSKeyId'

  KMSKeyArn:
    Condition: UseKMSEncryption
    Description: 'ARN of the KMS key used for encryption'
    Value: !GetAtt AnalyticsKMSKey.Arn
    Export:
      Name: !Sub '${AWS::StackName}-KMSKeyArn'

  # =============================================================================
  # Monitoring Outputs
  # =============================================================================
  DashboardUrl:
    Condition: UseDetailedMonitoring
    Description: 'URL to the CloudWatch dashboard'
    Value: !Sub 'https://${AWS::Region}.console.aws.amazon.com/cloudwatch/home?region=${AWS::Region}#dashboards:name=${ProjectName}-${Environment}-analytics-dashboard'
    Export:
      Name: !Sub '${AWS::StackName}-DashboardUrl'

  LambdaLogGroupName:
    Description: 'Name of the Lambda function log group'
    Value: !Ref LambdaLogGroup
    Export:
      Name: !Sub '${AWS::StackName}-LambdaLogGroup'

  BedrockLogGroupName:
    Description: 'Name of the Bedrock AgentCore log group'
    Value: !Ref BedrockLogGroup
    Export:
      Name: !Sub '${AWS::StackName}-BedrockLogGroup'

  # =============================================================================
  # Operational Outputs
  # =============================================================================
  DLQUrl:
    Description: 'URL of the SQS Dead Letter Queue'
    Value: !Ref AnalyticsDLQ
    Export:
      Name: !Sub '${AWS::StackName}-DLQUrl'

  DLQArn:
    Description: 'ARN of the SQS Dead Letter Queue'
    Value: !GetAtt AnalyticsDLQ.Arn
    Export:
      Name: !Sub '${AWS::StackName}-DLQArn'

  AlertTopicArn:
    Condition: IsProduction
    Description: 'ARN of the SNS topic for alerts'
    Value: !Ref AlertTopic
    Export:
      Name: !Sub '${AWS::StackName}-AlertTopic'

  # =============================================================================
  # Usage Instructions
  # =============================================================================
  TestCommand:
    Description: 'Sample curl command to test the analytics API'
    Value: !Sub |
      curl -X POST \
        '${AnalyticsAPI}.execute-api.${AWS::Region}.amazonaws.com/${ApiGatewayStage}/analytics' \
        -H 'Content-Type: application/json' \
        -d '{"query": "Analyze sales trends and provide insights on regional performance"}'

  DeploymentTimestamp:
    Description: 'Timestamp of the CloudFormation deployment'
    Value: !Sub '${AWS::Region}-${AWS::AccountId}-${AWS::StackName}'
    Export:
      Name: !Sub '${AWS::StackName}-DeploymentInfo'