"""
JSON to CSV Converter Lambda Function

This AWS Lambda function automatically converts JSON files to CSV format when
they are uploaded to an S3 bucket. The function is triggered by S3 events and
processes various JSON structures including arrays of objects, arrays of simple
values, and single objects.

Features:
- Handles multiple JSON data structures
- Generates CSV files with appropriate headers
- Comprehensive error handling and logging
- Supports nested object flattening (basic level)
- Configurable through environment variables

Author: Generated by Terraform
Version: 1.0.0
"""

import json
import csv
import boto3
import urllib.parse
import os
import logging
from io import StringIO
from typing import Any, Dict, List, Union

# Configure logging
logger = logging.getLogger()
logger.setLevel(logging.INFO)

# Initialize S3 client outside handler for connection reuse
s3_client = boto3.client('s3')

def lambda_handler(event: Dict[str, Any], context: Any) -> Dict[str, Any]:
    """
    Main Lambda handler function for JSON to CSV conversion.
    
    Args:
        event: S3 event notification containing bucket and object information
        context: Lambda runtime context object
        
    Returns:
        Dict containing status code and response message
        
    Raises:
        Exception: Re-raises any processing errors for Lambda retry mechanism
    """
    try:
        # Extract bucket and object key from S3 event
        record = event['Records'][0]
        bucket_name = record['s3']['bucket']['name']
        object_key = urllib.parse.unquote_plus(
            record['s3']['object']['key'], 
            encoding='utf-8'
        )
        
        logger.info(f"Processing file: {object_key} from bucket: {bucket_name}")
        
        # Validate file extension
        if not object_key.lower().endswith('.json'):
            logger.warning(f"Skipping non-JSON file: {object_key}")
            return {
                'statusCode': 200,
                'body': json.dumps(f'Skipped non-JSON file: {object_key}')
            }
        
        # Read JSON file from S3
        try:
            response = s3_client.get_object(Bucket=bucket_name, Key=object_key)
            json_content = response['Body'].read().decode('utf-8')
            
            # Log file size for monitoring
            file_size = len(json_content)
            logger.info(f"Processing file size: {file_size} bytes")
            
        except Exception as e:
            logger.error(f"Failed to read file {object_key}: {str(e)}")
            raise
        
        # Parse JSON content
        try:
            data = json.loads(json_content)
            logger.info(f"Successfully parsed JSON data. Type: {type(data).__name__}")
        except json.JSONDecodeError as e:
            logger.error(f"Invalid JSON format in file {object_key}: {str(e)}")
            raise ValueError(f"Invalid JSON format: {str(e)}")
        
        # Convert JSON to CSV
        csv_content = convert_json_to_csv(data, object_key)
        
        if csv_content is None:
            logger.warning(f"No CSV content generated for file: {object_key}")
            return {
                'statusCode': 400,
                'body': json.dumps(f'Unable to convert {object_key} to CSV')
            }
        
        # Generate output file name
        output_key = object_key.replace('.json', '.csv')
        output_bucket = os.environ.get('OUTPUT_BUCKET', '${output_bucket}')
        
        # Upload CSV to output bucket
        try:
            s3_client.put_object(
                Bucket=output_bucket,
                Key=output_key,
                Body=csv_content,
                ContentType='text/csv',
                Metadata={
                    'source-file': object_key,
                    'source-bucket': bucket_name,
                    'processed-by': 'json-csv-converter-lambda',
                    'conversion-timestamp': str(context.aws_request_id)
                }
            )
            
            logger.info(f"Successfully converted {object_key} to {output_key}")
            logger.info(f"CSV file size: {len(csv_content)} bytes")
            
        except Exception as e:
            logger.error(f"Failed to upload CSV file {output_key}: {str(e)}")
            raise
        
        return {
            'statusCode': 200,
            'body': json.dumps({
                'message': f'Successfully converted {object_key} to CSV',
                'input_file': object_key,
                'output_file': output_key,
                'output_bucket': output_bucket,
                'input_size_bytes': file_size,
                'output_size_bytes': len(csv_content)
            })
        }
        
    except Exception as e:
        error_message = f"Error processing file {object_key if 'object_key' in locals() else 'unknown'}: {str(e)}"
        logger.error(error_message)
        
        # Re-raise the exception to trigger Lambda retry mechanism
        raise Exception(error_message)

def convert_json_to_csv(data: Union[List, Dict, Any], filename: str) -> str:
    """
    Convert JSON data to CSV format based on data structure.
    
    Args:
        data: JSON data to convert (list, dict, or other)
        filename: Original filename for logging purposes
        
    Returns:
        String containing CSV data or None if conversion fails
    """
    csv_buffer = StringIO()
    
    try:
        if isinstance(data, list) and len(data) > 0:
            return handle_list_data(data, csv_buffer, filename)
        elif isinstance(data, dict):
            return handle_dict_data(data, csv_buffer, filename)
        else:
            return handle_simple_data(data, csv_buffer, filename)
            
    except Exception as e:
        logger.error(f"Error converting data to CSV: {str(e)}")
        return None

def handle_list_data(data: List, csv_buffer: StringIO, filename: str) -> str:
    """Handle conversion of list data to CSV."""
    first_item = data[0]
    
    if isinstance(first_item, dict):
        # Handle array of objects
        logger.info(f"Converting array of {len(data)} objects to CSV")
        
        # Get all possible fieldnames from all objects (handles inconsistent schemas)
        fieldnames = set()
        for item in data:
            if isinstance(item, dict):
                fieldnames.update(flatten_dict(item).keys())
        
        fieldnames = sorted(list(fieldnames))
        writer = csv.DictWriter(csv_buffer, fieldnames=fieldnames)
        writer.writeheader()
        
        # Write rows with flattened data
        for item in data:
            if isinstance(item, dict):
                flattened_item = flatten_dict(item)
                # Fill missing fields with empty strings
                row = {field: flattened_item.get(field, '') for field in fieldnames}
                writer.writerow(row)
            else:
                logger.warning(f"Inconsistent data types in array: {type(item)}")
                
    else:
        # Handle array of simple values
        logger.info(f"Converting array of {len(data)} simple values to CSV")
        writer = csv.writer(csv_buffer)
        writer.writerow(['value'])  # Header for simple values
        
        for item in data:
            writer.writerow([str(item)])
    
    return csv_buffer.getvalue()

def handle_dict_data(data: Dict, csv_buffer: StringIO, filename: str) -> str:
    """Handle conversion of dictionary data to CSV."""
    logger.info("Converting single object to CSV")
    
    flattened_data = flatten_dict(data)
    fieldnames = list(flattened_data.keys())
    
    writer = csv.DictWriter(csv_buffer, fieldnames=fieldnames)
    writer.writeheader()
    writer.writerow(flattened_data)
    
    return csv_buffer.getvalue()

def handle_simple_data(data: Any, csv_buffer: StringIO, filename: str) -> str:
    """Handle conversion of simple data types to CSV."""
    logger.info(f"Converting simple value ({type(data).__name__}) to CSV")
    
    writer = csv.writer(csv_buffer)
    writer.writerow(['value'])  # Header
    writer.writerow([str(data)])
    
    return csv_buffer.getvalue()

def flatten_dict(data: Dict, parent_key: str = '', separator: str = '.') -> Dict[str, Any]:
    """
    Flatten nested dictionary structures for CSV conversion.
    
    Args:
        data: Dictionary to flatten
        parent_key: Parent key for nested structures
        separator: Separator for nested key names
        
    Returns:
        Flattened dictionary with dot-notation keys
    """
    items = []
    
    for key, value in data.items():
        new_key = f"{parent_key}{separator}{key}" if parent_key else key
        
        if isinstance(value, dict):
            # Recursively flatten nested dictionaries
            items.extend(flatten_dict(value, new_key, separator).items())
        elif isinstance(value, list):
            # Handle lists by converting to string or flattening simple structures
            if len(value) > 0 and isinstance(value[0], dict):
                # For lists of objects, create indexed keys
                for i, item in enumerate(value):
                    if isinstance(item, dict):
                        indexed_key = f"{new_key}[{i}]"
                        items.extend(flatten_dict(item, indexed_key, separator).items())
                    else:
                        items.append((f"{new_key}[{i}]", str(item)))
            else:
                # For simple lists, join values with semicolon
                items.append((new_key, '; '.join(str(item) for item in value)))
        else:
            # Simple values
            items.append((new_key, value))
    
    return dict(items)

# Error handling for Lambda cold starts and other edge cases
def handle_lambda_timeout():
    """Handle Lambda timeout gracefully."""
    logger.warning("Lambda function approaching timeout")
    return {
        'statusCode': 408,  # Request Timeout
        'body': json.dumps('Function timed out during processing')
    }

# Health check function for monitoring
def health_check() -> Dict[str, Any]:
    """Simple health check for Lambda function."""
    try:
        # Test S3 connectivity
        s3_client.list_buckets()
        
        return {
            'statusCode': 200,
            'body': json.dumps({
                'status': 'healthy',
                'timestamp': str(context.aws_request_id if 'context' in globals() else 'unknown'),
                's3_connectivity': 'ok'
            })
        }
    except Exception as e:
        return {
            'statusCode': 500,
            'body': json.dumps({
                'status': 'unhealthy',
                'error': str(e)
            })
        }