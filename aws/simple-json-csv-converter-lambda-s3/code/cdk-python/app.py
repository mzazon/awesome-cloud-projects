#!/usr/bin/env python3
"""
AWS CDK Python application for Simple JSON to CSV Converter with Lambda and S3.

This CDK application creates a serverless data processing pipeline that automatically
converts JSON files to CSV format when uploaded to an S3 bucket.

Architecture:
- Input S3 bucket for JSON files
- Output S3 bucket for CSV files
- Lambda function for JSON to CSV conversion
- S3 event trigger for automatic processing
- CloudWatch logs for monitoring

Author: Generated by CDK Python Application Generator
"""

import os
from typing import Dict, Any

import aws_cdk as cdk
from aws_cdk import (
    Stack,
    StackProps,
    Environment,
    Tags,
    RemovalPolicy,
    CfnOutput,
    Duration
)
from aws_cdk import aws_lambda as _lambda
from aws_cdk import aws_s3 as s3
from aws_cdk import aws_s3_notifications as s3n
from aws_cdk import aws_iam as iam
from aws_cdk import aws_logs as logs
from constructs import Construct
from cdk_nag import AwsSolutionsChecks, NagSuppressions


class JsonCsvConverterStack(Stack):
    """
    CDK Stack for JSON to CSV converter using Lambda and S3.
    
    This stack creates a complete serverless data processing pipeline with:
    - Secure S3 buckets with encryption and versioning
    - Lambda function with proper IAM permissions
    - Event-driven processing architecture
    - CloudWatch logging and monitoring
    """

    def __init__(self, scope: Construct, construct_id: str, **kwargs) -> None:
        super().__init__(scope, construct_id, **kwargs)

        # Generate unique suffix for resource names
        unique_suffix = self.account + "-" + self.region

        # Create S3 bucket for JSON input files
        self.input_bucket = s3.Bucket(
            self, "InputBucket",
            bucket_name=f"json-input-{unique_suffix}"[:63].lower(),
            encryption=s3.BucketEncryption.S3_MANAGED,
            versioned=True,
            block_public_access=s3.BlockPublicAccess.BLOCK_ALL,
            enforce_ssl=True,
            removal_policy=RemovalPolicy.DESTROY,
            auto_delete_objects=True,
            lifecycle_rules=[
                s3.LifecycleRule(
                    id="DeleteOldVersions",
                    enabled=True,
                    noncurrent_version_expiration=Duration.days(30)
                )
            ]
        )

        # Create S3 bucket for CSV output files
        self.output_bucket = s3.Bucket(
            self, "OutputBucket",
            bucket_name=f"csv-output-{unique_suffix}"[:63].lower(),
            encryption=s3.BucketEncryption.S3_MANAGED,
            versioned=True,
            block_public_access=s3.BlockPublicAccess.BLOCK_ALL,
            enforce_ssl=True,
            removal_policy=RemovalPolicy.DESTROY,
            auto_delete_objects=True,
            lifecycle_rules=[
                s3.LifecycleRule(
                    id="DeleteOldVersions",
                    enabled=True,
                    noncurrent_version_expiration=Duration.days(30)
                )
            ]
        )

        # Create IAM role for Lambda function
        lambda_role = iam.Role(
            self, "LambdaExecutionRole",
            assumed_by=iam.ServicePrincipal("lambda.amazonaws.com"),
            managed_policies=[
                iam.ManagedPolicy.from_aws_managed_policy_name(
                    "service-role/AWSLambdaBasicExecutionRole"
                )
            ],
            inline_policies={
                "S3AccessPolicy": iam.PolicyDocument(
                    statements=[
                        # Read access to input bucket
                        iam.PolicyStatement(
                            effect=iam.Effect.ALLOW,
                            actions=["s3:GetObject"],
                            resources=[f"{self.input_bucket.bucket_arn}/*"]
                        ),
                        # Write access to output bucket
                        iam.PolicyStatement(
                            effect=iam.Effect.ALLOW,
                            actions=["s3:PutObject"],
                            resources=[f"{self.output_bucket.bucket_arn}/*"]
                        )
                    ]
                )
            }
        )

        # Create CloudWatch log group for Lambda function
        log_group = logs.LogGroup(
            self, "LambdaLogGroup",
            log_group_name=f"/aws/lambda/json-csv-converter-{unique_suffix}",
            retention=logs.RetentionDays.ONE_WEEK,
            removal_policy=RemovalPolicy.DESTROY
        )

        # Create Lambda function for JSON to CSV conversion
        self.lambda_function = _lambda.Function(
            self, "JsonCsvConverterFunction",
            function_name=f"json-csv-converter-{unique_suffix}",
            runtime=_lambda.Runtime.PYTHON_3_12,
            handler="index.lambda_handler",
            code=_lambda.Code.from_inline(self._get_lambda_code()),
            role=lambda_role,
            timeout=Duration.seconds(60),
            memory_size=256,
            environment={
                "OUTPUT_BUCKET": self.output_bucket.bucket_name,
                "LOG_LEVEL": "INFO"
            },
            log_group=log_group,
            reserved_concurrent_executions=10,
            retry_attempts=2
        )

        # Add S3 event notification to trigger Lambda
        self.input_bucket.add_event_notification(
            s3.EventType.OBJECT_CREATED,
            s3n.LambdaDestination(self.lambda_function),
            s3.NotificationKeyFilter(suffix=".json")
        )

        # Add CDK Nag suppressions for acceptable security practices
        NagSuppressions.add_stack_suppressions(
            self,
            [
                {
                    "id": "AwsSolutions-IAM4",
                    "reason": "Using AWS managed policy AWSLambdaBasicExecutionRole "
                             "for standard Lambda execution permissions"
                },
                {
                    "id": "AwsSolutions-IAM5",
                    "reason": "S3 object-level permissions require wildcard for "
                             "processing files with dynamic names"
                },
                {
                    "id": "AwsSolutions-S3-1",
                    "reason": "S3 server access logging not required for this "
                             "simple processing pipeline"
                }
            ]
        )

        # Stack outputs
        CfnOutput(
            self, "InputBucketName",
            value=self.input_bucket.bucket_name,
            description="Name of the S3 bucket for uploading JSON files",
            export_name=f"{self.stack_name}-InputBucket"
        )

        CfnOutput(
            self, "OutputBucketName",
            value=self.output_bucket.bucket_name,
            description="Name of the S3 bucket where CSV files are stored",
            export_name=f"{self.stack_name}-OutputBucket"
        )

        CfnOutput(
            self, "LambdaFunctionName",
            value=self.lambda_function.function_name,
            description="Name of the Lambda function processing JSON files",
            export_name=f"{self.stack_name}-LambdaFunction"
        )

    def _get_lambda_code(self) -> str:
        """
        Returns the Lambda function code for JSON to CSV conversion.
        
        The function handles multiple JSON structures:
        - Arrays of objects (most common)
        - Single objects
        - Arrays of simple values
        
        Returns:
            str: Python code for the Lambda function
        """
        return '''
import json
import csv
import boto3
import urllib.parse
import os
import logging
from io import StringIO
from typing import Any, Dict, List, Union

# Configure logging
logger = logging.getLogger()
logger.setLevel(os.environ.get('LOG_LEVEL', 'INFO'))

# Initialize S3 client
s3_client = boto3.client('s3')

def lambda_handler(event: Dict[str, Any], context: Any) -> Dict[str, Any]:
    """
    AWS Lambda function to convert JSON files to CSV format.
    
    Args:
        event: S3 event notification containing bucket and object information
        context: Lambda runtime context
        
    Returns:
        Dict containing status code and response message
    """
    try:
        # Extract bucket and object information from S3 event
        record = event['Records'][0]
        bucket = record['s3']['bucket']['name']
        key = urllib.parse.unquote_plus(
            record['s3']['object']['key'], 
            encoding='utf-8'
        )
        
        logger.info(f"Processing file: {key} from bucket: {bucket}")
        
        # Validate file extension
        if not key.lower().endswith('.json'):
            logger.warning(f"Skipping non-JSON file: {key}")
            return {
                'statusCode': 200,
                'body': json.dumps(f'Skipped non-JSON file: {key}')
            }
        
        # Read JSON file from S3
        try:
            response = s3_client.get_object(Bucket=bucket, Key=key)
            json_content = response['Body'].read().decode('utf-8')
            logger.info(f"Successfully read {len(json_content)} characters from {key}")
        except Exception as e:
            logger.error(f"Failed to read file {key}: {str(e)}")
            raise
        
        # Parse JSON content
        try:
            data = json.loads(json_content)
            logger.info(f"Successfully parsed JSON data, type: {type(data)}")
        except json.JSONDecodeError as e:
            logger.error(f"Invalid JSON in file {key}: {str(e)}")
            raise ValueError(f"Invalid JSON format: {str(e)}")
        
        # Convert JSON to CSV based on data structure
        csv_buffer = StringIO()
        
        if isinstance(data, list) and len(data) > 0:
            # Handle array of objects or values
            if isinstance(data[0], dict):
                # Array of objects - use keys as headers
                fieldnames = data[0].keys()
                writer = csv.DictWriter(csv_buffer, fieldnames=fieldnames)
                writer.writeheader()
                writer.writerows(data)
                logger.info(f"Converted array of {len(data)} objects with fields: {list(fieldnames)}")
            else:
                # Array of simple values
                writer = csv.writer(csv_buffer)
                writer.writerow(['value'])  # Header for simple values
                for item in data:
                    writer.writerow([item])
                logger.info(f"Converted array of {len(data)} simple values")
        elif isinstance(data, dict):
            # Single object
            writer = csv.DictWriter(csv_buffer, fieldnames=data.keys())
            writer.writeheader()
            writer.writerow(data)
            logger.info(f"Converted single object with fields: {list(data.keys())}")
        elif isinstance(data, list) and len(data) == 0:
            # Empty array
            writer = csv.writer(csv_buffer)
            writer.writerow(['value'])  # Default header
            logger.warning(f"Processed empty array from {key}")
        else:
            # Unsupported structure
            raise ValueError(f"Unsupported JSON structure: {type(data)}")
        
        # Generate output file name
        output_key = key.rsplit('.json', 1)[0] + '.csv'
        output_bucket = os.environ['OUTPUT_BUCKET']
        
        # Upload CSV to output bucket
        try:
            s3_client.put_object(
                Bucket=output_bucket,
                Key=output_key,
                Body=csv_buffer.getvalue(),
                ContentType='text/csv',
                Metadata={
                    'source-file': key,
                    'conversion-timestamp': str(context.aws_request_id)
                }
            )
            logger.info(f"Successfully uploaded CSV to s3://{output_bucket}/{output_key}")
        except Exception as e:
            logger.error(f"Failed to upload CSV file: {str(e)}")
            raise
        
        # Return success response
        success_message = f'Successfully converted {key} to {output_key}'
        logger.info(success_message)
        
        return {
            'statusCode': 200,
            'body': json.dumps({
                'message': success_message,
                'input_file': key,
                'output_file': output_key,
                'output_bucket': output_bucket
            })
        }
        
    except Exception as e:
        error_message = f"Error processing file {key if 'key' in locals() else 'unknown'}: {str(e)}"
        logger.error(error_message)
        
        # Return error response
        return {
            'statusCode': 500,
            'body': json.dumps({
                'error': error_message,
                'input_file': key if 'key' in locals() else 'unknown'
            })
        }
'''


def main() -> None:
    """
    Main function to create and deploy the CDK application.
    """
    app = cdk.App()
    
    # Get stack name from context or use default
    stack_name = app.node.try_get_context("stack_name") or "JsonCsvConverterStack"
    
    # Create the stack
    stack = JsonCsvConverterStack(
        app, 
        stack_name,
        env=Environment(
            account=os.environ.get('CDK_DEFAULT_ACCOUNT'),
            region=os.environ.get('CDK_DEFAULT_REGION', 'us-east-1')
        ),
        description="Simple JSON to CSV converter with Lambda and S3 - "
                   "Serverless data processing pipeline"
    )
    
    # Add tags to all resources
    Tags.of(stack).add("Project", "JsonCsvConverter")
    Tags.of(stack).add("Environment", app.node.try_get_context("environment") or "dev")
    Tags.of(stack).add("ManagedBy", "CDK")
    Tags.of(stack).add("Recipe", "simple-json-csv-converter-lambda-s3")
    
    # Apply CDK Nag for security best practices
    if app.node.try_get_context("enable_cdk_nag") != "false":
        AwsSolutionsChecks.check(stack)
    
    app.synth()


if __name__ == "__main__":
    main()